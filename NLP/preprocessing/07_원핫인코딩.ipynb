{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ’¡ 07. ì›-í•« ì¸ì½”ë”© (One-hot Encoding): ìŠ¤ìœ„ì¹˜ ì¼œê¸°\n",
                "\n",
                "## 1. ê°œìš”\n",
                "ì•ì„œ ë‹¨ì–´ë¥¼ ìˆ«ì(1, 2, 3...)ë¡œ ë°”ê¿¨ìŠµë‹ˆë‹¤. \n",
                "ê·¸ëŸ°ë° ìˆ«ìë¡œ ê·¸ëƒ¥ ë‘ë©´ AIê°€ **\"3ë²ˆ ë‹¨ì–´ëŠ” 1ë²ˆ ë‹¨ì–´ë³´ë‹¤ 3ë°° ë” í¬ë„¤?\"**ë¼ê³  ì˜¤í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "ì´ ì˜¤í•´ë¥¼ ë§‰ê¸° ìœ„í•´, ìˆ«ìì˜ í¬ê¸°ë¥¼ ì—†ì• ê³  **ìœ„ì¹˜**ë¡œë§Œ í‘œì‹œí•˜ëŠ” ë°©ë²•ì´ ë°”ë¡œ **ì›-í•« ì¸ì½”ë”©**ì…ë‹ˆë‹¤.\n",
                "\n",
                "> **ì‰¬ìš´ ë¹„ìœ **:  \n",
                "> - **ì •ìˆ˜ ì¸ì½”ë”©**: 1ë²ˆ, 2ë²ˆ, 3ë²ˆ ì„ ìˆ˜ (ë“±ìˆ˜ê°€ ì•„ë‹˜ì—ë„ ë“±ìˆ˜ì²˜ëŸ¼ ë³´ì„)\n",
                "> - **ì›-í•« ì¸ì½”ë”©**: ì „êµ¬ 100ê°œ ì¤‘ **í•´ë‹¹í•˜ëŠ” ì „êµ¬ 1ê°œë§Œ ìŠ¤ìœ„ì¹˜ë¥¼ ì¼œëŠ”(ON)** ë°©ì‹.\n",
                ">   - ì‚¬ê³¼ â¡ï¸ `[1, 0, 0]`\n",
                ">   - ë°”ë‚˜ë‚˜ â¡ï¸ `[0, 1, 0]`\n",
                ">   - ë”¸ê¸° â¡ï¸ `[0, 0, 1]`\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ì£¼ìš” ê°œë… ë° ì¥ë‹¨ì \n",
                "\n",
                "### íŠ¹ì§•\n",
                "- ë‹¨ì–´ ì§‘í•©ì˜ í¬ê¸°ë§Œí¼ ê¸¸ì­‰í•œ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
                "- ë”± 1ê°œì˜ ê°’ë§Œ `1`(Hot)ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ `0`(Cold)ì…ë‹ˆë‹¤.\n",
                "\n",
                "### ğŸ’¥ ì¹˜ëª…ì ì¸ ë‹¨ì  (ë©”ëª¨ë¦¬ í­ë°œ)\n",
                "- ë‹¨ì–´ê°€ 10,000ê°œ ìˆìœ¼ë©´, ë²¡í„° ê¸¸ì´ë„ 10,000ì¹¸ì´ ë©ë‹ˆë‹¤.\n",
                "- `[0, 0, ..., 1, ..., 0]` : 0ì´ ë„ˆë¬´ ë§ì•„ì„œ ì €ì¥ ê³µê°„ ë‚­ë¹„ê°€ ì‹¬í•©ë‹ˆë‹¤. (**í¬ì†Œ í–‰ë ¬**ì´ë¼ê³  í•©ë‹ˆë‹¤)\n",
                "- ë‹¨ì–´ ê°„ì˜ ê´€ë ¨ì„±ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ê³¼ì™€ ë°°ê°€ ë¹„ìŠ·í•œì§€ ëª¨ë¦„)\n",
                "\n",
                "> ê·¸ë˜ì„œ ìš”ì¦˜ì€ **ì„ë² ë”©(Embedding)**ì´ë¼ëŠ” ë” ì¢‹ì€ ë°©ë²•ì„ ì£¼ë¡œ ì“°ì§€ë§Œ, ê¸°ë³¸ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë°°ì›Œë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ì‹¤ìŠµí•´ë³´ê¸°\n",
                "\n",
                "ë¨¼ì € í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ë¬¸ì¥ì„ ìª¼ê°œê³  ì •ìˆ˜ ì¸ì½”ë”©ì„ ë§Œë“­ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from konlpy.tag import Okt\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "\n",
                "text = \"ì˜¤ëŠ˜ ì ì‹¬ì€ ë§›ìˆëŠ” ê¹€ë°¥ì´ë‹¤\"\n",
                "\n",
                "# 1. í† í°í™”\n",
                "okt = Okt()\n",
                "tokens = okt.morphs(text)\n",
                "print(\"í† í°:\", tokens)\n",
                "\n",
                "# 2. ì •ìˆ˜ ì¸ì½”ë”©\n",
                "tokenizer = Tokenizer()\n",
                "tokenizer.fit_on_texts([tokens]) # ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë„£ì–´ì•¼ í•¨\n",
                "print(\"ë‹¨ì–´ì¥:\", tokenizer.word_index)\n",
                "\n",
                "sequences = tokenizer.texts_to_sequences([tokens])[0]\n",
                "print(\"ì •ìˆ˜ ì¸ì½”ë”©:\", sequences)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. `to_categorical`ë¡œ ë³€í™˜í•˜ê¸°\n",
                "Kerasì˜ `to_categorical` í•¨ìˆ˜ë¥¼ ì“°ë©´ ì •ìˆ˜ë¥¼ ì›-í•« ë²¡í„°ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë¬¸ì¥ì˜ ê¸¸ì´ëŠ” 4ê°œ, ë‹¨ì–´ ì¢…ë¥˜ëŠ” 4ê°œì…ë‹ˆë‹¤.\n",
                "# to_categorical(ì •ìˆ˜ ë°ì´í„°, ì „ì²´ í´ë˜ìŠ¤ ê°œìˆ˜)\n",
                "# ì£¼ì˜: ì¸ë±ìŠ¤ê°€ 1ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ, í´ë˜ìŠ¤ ê°œìˆ˜ëŠ” (ê°€ì¥ í° ë²ˆí˜¸ + 1)ì´ì–´ì•¼ ì•ˆì „í•©ë‹ˆë‹¤.\n",
                "\n",
                "vocab_size = len(tokenizer.word_index) + 1\n",
                "one_hot = to_categorical(sequences, num_classes=vocab_size)\n",
                "\n",
                "print(\"ê²°ê³¼ í–‰ë ¬ í¬ê¸°:\", one_hot.shape)\n",
                "print(\"\\nì›-í•« ë²¡í„°ë“¤:\")\n",
                "print(one_hot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ê²°ê³¼ í•´ì„\n",
                "- `[0. 1. 0. 0. 0.]` â¡ï¸ 1ë²ˆ ë‹¨ì–´ ('ì˜¤ëŠ˜')\n",
                "- `[0. 0. 1. 0. 0.]` â¡ï¸ 2ë²ˆ ë‹¨ì–´ ('ì ì‹¬')\n",
                "- ë§¨ ì•ì˜ `0.`ì€ ì¸ë±ìŠ¤ 0ë²ˆ(íŒ¨ë”©ìš©) ìë¦¬ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ì›-í•« ì¸ì½”ë”©ì˜ í•œê³„ ì§ì ‘ í™•ì¸í•˜ê¸°\n",
                "ë‹¨ì–´ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°(ë‚´ì )í•´ë³´ë©´ ë¬´ì¡°ê±´ 0ì´ ë‚˜ì˜µë‹ˆë‹¤. ì„œë¡œ ì™„ì „íˆ ë‚¨ë‚¨ì´ë¼ëŠ” ëœ»ì´ì£ ."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "vec1 = one_hot[0] # ì˜¤ëŠ˜\n",
                "vec2 = one_hot[1] # ì ì‹¬\n",
                "\n",
                "# ë‚´ì (dot product) ê³„ì‚°\n",
                "similarity = np.dot(vec1, vec2)\n",
                "print(f\"'ì˜¤ëŠ˜'ê³¼ 'ì ì‹¬'ì˜ ìœ ì‚¬ë„: {similarity}\")\n",
                "\n",
                "# ê²°ê³¼ê°€ 0ì´ë¼ëŠ” ê²ƒì€ 'ì „í˜€ ê´€ê³„ì—†ìŒ'ì„ ëœ»í•©ë‹ˆë‹¤.\n",
                "# ì‹¤ì œë¡œëŠ” ê´€ê³„ê°€ ìˆì§€ë§Œ, ì›-í•« ì¸ì½”ë”©ì€ ì´ê±¸ í‘œí˜„ ëª» í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ë§ˆë¬´ë¦¬\n",
                "\n",
                "- **One-hot Encoding**: ìŠ¤ìœ„ì¹˜ë¥¼ í•˜ë‚˜ë§Œ ì¼œëŠ” ë°©ì‹.\n",
                "- **ë‹¨ì **: ê³µê°„ ë‚­ë¹„ ì‹¬í•¨, ë‹¨ì–´ì˜ ì†ëœ»ì„ ëª¨ë¦„.\n",
                "- **í•´ê²°ì±…**: ë‹¤ìŒì— ë°°ìš¸ **ì›Œë“œ ì„ë² ë”©(Word Embedding)**!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}