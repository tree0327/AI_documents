{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ§¹ 02. ì •ì œ(Cleansing) & ì •ê·œí™”(Normalization): ë°ì´í„° ëª©ìš•ì‹œí‚¤ê¸°\n",
                "\n",
                "## 1. ê°œìš”\n",
                "í† í°í™”ë¡œ ë¬¸ì¥ì„ ì˜ë¼ëƒˆì§€ë§Œ, ì•„ì§ **ì§€ì €ë¶„í•œ ê²ƒë“¤**ì´ ë§ì´ ë¬»ì–´ìˆìŠµë‹ˆë‹¤. \n",
                "ë¶„ì„ì— ë°©í•´ë˜ëŠ” ê²ƒë“¤ì„ ì”»ì–´ë‚´ê³ (ì •ì œ), ëª¨ì–‘ì„ ì˜ˆì˜ê²Œ ë‹¤ë“¬ëŠ”(ì •ê·œí™”) ê³¼ì •ì„ ë°°ì›Œë´…ì‹œë‹¤.\n",
                "\n",
                "### ğŸ”‘ í•µì‹¬ ìš©ì–´ ì‚¬ì „\n",
                "| ìš©ì–´ | ëœ» | ë¹„ìœ  |\n",
                "|:---:|:---:|:---:|\n",
                "| **Noise (ë…¸ì´ì¦ˆ)** | ë¶„ì„ì„ ë°©í•´í•˜ëŠ” ë°©í•´ê¾¼ ë°ì´í„° | í™ ë¬»ì€ ë‹¹ê·¼ì˜ 'í™' |\n",
                "| **Cleansing (ì •ì œ)** | ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ëŠ” ì‘ì—… | ë‹¹ê·¼ ì”»ê¸° |\n",
                "| **Normalization (ì •ê·œí™”)** | í‘œí˜„ ë°©ë²•ì„ í•˜ë‚˜ë¡œ í†µì¼í•˜ëŠ” ì‘ì—… | ì‚ëš¤ë¹¼ëš¤í•œ ë‹¹ê·¼ì„ ì±„ì°ê¸°ë¡œ í†µì¼í•˜ê¸° |\n",
                "| **Stopwords (ë¶ˆìš©ì–´)** | ë¬¸ë²•ì ìœ¼ë¡œ í•„ìš”í•˜ì§€ë§Œ ì˜ë¯¸ê°€ ì—†ëŠ” ë‹¨ì–´ (ì€, ëŠ”, a, the) | í¬ì¥ì§€ (ë‚´ìš©ë¬¼ ì•„ë‹˜) |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ì •ì œì™€ ì •ê·œí™”, ë­ê°€ ë‹¤ë¥¸ê°€ìš”?\n",
                "\n",
                "| êµ¬ë¶„ | ëª©í‘œ | ì˜ˆì‹œ |\n",
                "|:---:|:---:|:---|\n",
                "| **ì •ì œ (Cleansing)** | **\"ë”ëŸ¬ìš´ ê²ƒ ë²„ë¦¬ê¸°\"** | íŠ¹ìˆ˜ë¬¸ì(`!@#`), ì´ëª¨ì§€(`ğŸ˜€`), ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì‚­ì œ |\n",
                "| **ì •ê·œí™” (Normalization)** | **\"ê°™ì€ ê±´ í•˜ë‚˜ë¡œ ëª¨ìœ¼ê¸°\"** | `US`ì™€ `USA`ëŠ” ê°™ì€ ëœ»ì´ë‹ˆ `USA`ë¡œ í†µì¼! `Goooooood` -> `Good` |\n",
                "\n",
                "> **ëª©í‘œ**: AIê°€ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ê¹”ë”í•˜ê³  í†µì¼ëœ ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. ê·œì¹™ ê¸°ë°˜ ì •ê·œí™” (Rule-based Normalization)\n",
                "\n",
                "\"Aê°€ ë‚˜ì˜¤ë©´ Bë¡œ ë°”ê¿”ë¼!\"ë¼ê³  ìš°ë¦¬ê°€ ì§ì ‘ ê·œì¹™ì„ ì •í•´ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
                "ê°€ì¥ ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ê°€ í™•ì‹¤í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì˜ˆì‹œ ë¬¸ì¥: ì˜¤íƒ€(Kimdom)ì™€ ì•½ì–´(UK)ê°€ ì„ì—¬ ìˆì–´ì„œ ì§€ì €ë¶„í•©ë‹ˆë‹¤.\n",
                "text = 'The United Kimdom and UK have a long history together. Uh-oh! Something went wrong, uhoh'\n",
                "\n",
                "# 1. replace í•¨ìˆ˜ë¡œ ì§ì ‘ ë°”ê¿”ì£¼ê¸°\n",
                "# 'United Kimdom' (ì˜¤íƒ€) -> 'UK' (í‘œì¤€ ë‹¨ì–´)ë¡œ í†µí•©\n",
                "# 'Uh-oh' -> 'uhoh' (í‘œê¸° í†µì¼)\n",
                "transformed_text = text.replace(\"United Kimdom\", 'UK').replace('Uh-oh', 'uhoh')\n",
                "\n",
                "print(\"ì›ë³¸:\", text)\n",
                "print(\"ìˆ˜ì •:\", transformed_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ëŒ€ì†Œë¬¸ì í†µí•© (Case Folding)\n",
                "ì˜ì–´ì—ì„œ `Apple`ê³¼ `apple`ì€ ì‚¬ì‹¤ ê°™ì€ ì‚¬ê³¼ì…ë‹ˆë‹¤.\n",
                "ì»´í“¨í„°ëŠ” ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ ì™„ì „ ë‹¤ë¥¸ ê¸€ìë¡œ ì¸ì‹í•˜ê¸° ë•Œë¬¸ì—, ë³´í†µ **ëª¨ë‘ ì†Œë¬¸ìë¡œ** ë°”ê¿‰ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# .lower() í•¨ìˆ˜: ëª¨ë“  ì˜ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë³€ì‹ ì‹œí‚µë‹ˆë‹¤.\n",
                "lower_text = transformed_text.lower()\n",
                "print(lower_text)\n",
                "\n",
                "# ì£¼ì˜: 'US'(ë¯¸êµ­)ë¥¼ 'us'(ìš°ë¦¬)ë¡œ ë°”ê¾¸ë©´ ì˜ë¯¸ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ ì¡°ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. ì •ì œ (Cleansing): ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì†ì•„ë‚´ê¸°\n",
                "\n",
                "ë¶„ì„í•˜ëŠ” ë° ë„ì›€ì´ ì•ˆ ë˜ëŠ” ë‹¨ì–´ë“¤ì„ ê³¼ê°í•˜ê²Œ ë²„ë¦½ë‹ˆë‹¤.\n",
                "\n",
                "1. **ë“±ì¥ ë¹ˆë„ê°€ ë„ˆë¬´ ì ì€ ë‹¨ì–´**: 10ë§Œ ê°œ ê¸€ ì¤‘ ë”± 1ë²ˆ ë‚˜ì˜¨ ë‹¨ì–´ëŠ” íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë‹ˆ ë²„ë¦½ë‹ˆë‹¤.\n",
                "2. **ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´**: ì˜ì–´ì˜ `a`, `I` ê°™ì€ ê±´ í° ì˜ë¯¸ê°€ ì—†ì„ ë•Œê°€ ë§ìŠµë‹ˆë‹¤.\n",
                "3. **ë¶ˆìš©ì–´ (Stopword)**: `is`, `the`, `that` ì²˜ëŸ¼ ë¬¸ë²•ì ìœ¼ë¡  ì¤‘ìš”í•˜ì§€ë§Œ ëœ»ì€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ë²„ë¦½ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4-1. ë„ˆë¬´ ë“œë¬¸ ë‹¨ì–´ ì§€ìš°ê¸° (ë¹ˆë„ìˆ˜ ê¸°ë°˜)\n",
                "`Counter` ë„êµ¬ë¥¼ ì“°ë©´ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì„¸ì–´ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.tokenize import word_tokenize\n",
                "from collections import Counter  # ê°œìˆ˜ ì„¸ì£¼ëŠ” ë„êµ¬\n",
                "\n",
                "# ì˜ˆì‹œ ë¬¸ì¥\n",
                "text = \"The quick brown fox jumps over the lazy dog. The fox is quick and agile\"\n",
                "\n",
                "# 1. ì¼ë‹¨ ë‹¨ì–´ë¡œ ìª¼ê°­ë‹ˆë‹¤ (í† í°í™”)\n",
                "tokens = word_tokenize(text)\n",
                "\n",
                "# 2. ê°œìˆ˜ë¥¼ ì…‰ë‹ˆë‹¤\n",
                "vocab = Counter(tokens)\n",
                "print(\"ë‹¨ì–´ë³„ ê°œìˆ˜:\", vocab)\n",
                "\n",
                "# 3. 2ë²ˆ ì´ìƒ ë‚˜ì˜¨ ë‹¨ì–´ë§Œ ë‚¨ê²¨ë´…ì‹œë‹¤.\n",
                "filtered_tokens = []\n",
                "for token in tokens:\n",
                "    if vocab[token] >= 2:\n",
                "        filtered_tokens.append(token)\n",
                "\n",
                "print(\"------------------\")\n",
                "print(\"ë‚¨ì€ ë‹¨ì–´ë“¤:\", filtered_tokens)\n",
                "# 'The', 'quick', 'fox' ì²˜ëŸ¼ 2ë²ˆ ì´ìƒ ë‚˜ì˜¨ ê²ƒë§Œ ì‚´ì•„ë‚¨ì•˜ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4-2. ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì§€ìš°ê¸° (ê¸¸ì´ ê¸°ë°˜)\n",
                "ì˜ì–´ì—ì„œëŠ” ê¸¸ì´ê°€ 1~2ê¸€ìì¸ ë‹¨ì–´(a, at, in, on)ë“¤ì´ ë³„ë¡œ ì¤‘ìš”í•˜ì§€ ì•Šì„ ë•Œê°€ ë§ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë‹¨ì–´ ê¸¸ì´ê°€ 3ê¸€ì ì´ìƒì¸ ê²ƒë§Œ ë‚¨ê¸°ê¸°!\n",
                "# ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜(List Comprehension)ì´ë¼ëŠ” íŒŒì´ì¬ ë¬¸ë²•ì„ ì“°ë©´ í•œ ì¤„ë¡œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
                "\n",
                "long_words = [word for word in tokens if len(word) >= 3]\n",
                "\n",
                "print(\"3ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë“¤:\", long_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4-3. ë¶ˆìš©ì–´(Stopwords) ì œê±°í•˜ê¸°\n",
                "**\"ì´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ê±´ ë‹¤ ì§€ì›Œ!\"** ë¼ê³  ë¯¸ë¦¬ ì •í•´ë‘” ëª©ë¡ì„ **ë¶ˆìš©ì–´ ì‚¬ì „**ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
                "NLTKì—ì„œ ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì œê³µí•´ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# ë¶ˆìš©ì–´ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ\n",
                "nltk.download('stopwords')\n",
                "\n",
                "# ì˜ì–´ ë¶ˆìš©ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
                "stop_words_list = stopwords.words('english')\n",
                "\n",
                "print(f\"ì˜ì–´ ë¶ˆìš©ì–´ ê°œìˆ˜: {len(stop_words_list)}ê°œ\")\n",
                "print(\"ì˜ˆì‹œ 10ê°œ:\", stop_words_list[:10])\n",
                "# i, me, my, myself... ê°™ì€ ë‹¨ì–´ë“¤ì´ ë“¤ì–´ìˆë„¤ìš”!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë¬¸ì¥ì—ì„œ ë¶ˆìš©ì–´ ë¹¼ê³  ë‚¨ê¸°ê¸°\n",
                "text = \"The quick brown fox jumps over the lazy dog.\"\n",
                "tokens = word_tokenize(text)\n",
                "\n",
                "result = []\n",
                "for word in tokens:\n",
                "    # ì†Œë¬¸ìë¡œ ë°”ê¿”ì„œ ë¹„êµí•´ì•¼ ì •í™•í•©ë‹ˆë‹¤ (The != the)\n",
                "    if word.lower() not in stop_words_list:\n",
                "        result.append(word)\n",
                "\n",
                "print(\"ì›ë³¸:\", tokens)\n",
                "print(\"ë¶ˆìš©ì–´ ì œê±° í›„:\", result)\n",
                "# 'The', 'over', 'the' ê°™ì€ ë‹¨ì–´ë“¤ì´ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. í•œêµ­ì–´ ë¶ˆìš©ì–´ ì²˜ë¦¬\n",
                "\n",
                "í•œêµ­ì–´ëŠ” **ì¡°ì‚¬(ì€/ëŠ”/ì´/ê°€/ì„/ë¥¼)**ë‚˜ **ì ‘ì†ì‚¬(ê·¸ë¦¬ê³ /í•˜ì§€ë§Œ)**, **ì˜ì¡´ëª…ì‚¬(ê²ƒ/ìˆ˜/ë“±)** ë“±ì„ ì£¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.\n",
                "í•œêµ­ì–´ëŠ” NLTKì²˜ëŸ¼ ë”± ì •í•´ì§„ ë¶ˆìš©ì–´ ì‚¬ì „ì´ ì—†ì–´ì„œ, ë³´í†µ ê°œë°œìê°€ ì§ì ‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê±°ë‚˜ ì¸í„°ë„·ì— ê³µìœ ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ ì”ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from konlpy.tag import Okt\n",
                "\n",
                "okt = Okt()\n",
                "korean_text = \"ì´ê²ƒì€ ì˜ˆì‹œ ë¬¸ì¥ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì €ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ê·¸ì € ì˜ˆì‹œì¼ ë¿ì…ë‹ˆë‹¤.\"\n",
                "\n",
                "# 1. í˜•íƒœì†Œ ë¶„ì„\n",
                "morphs = okt.morphs(korean_text)\n",
                "print(\"ëª¨ë“  í˜•íƒœì†Œ:\", morphs)\n",
                "\n",
                "# 2. ë‚˜ë§Œì˜ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°\n",
                "my_stopwords = ['ì€', 'ì…ë‹ˆë‹¤', '.', 'ê·¸ë¦¬ê³ ', 'ì €ê²ƒ', 'ê·¸ì €', 'ì¼', 'ë¿']\n",
                "\n",
                "# 3. ë¶ˆìš©ì–´ ì œê±°í•˜ê¸°\n",
                "clean_morphs = [word for word in morphs if word not in my_stopwords]\n",
                "\n",
                "print(\"ì²­ì†Œ ëë‚œ í˜•íƒœì†Œ:\", clean_morphs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### íŒ: ë¶ˆìš©ì–´ íŒŒì¼(.txt)ë¡œ ê´€ë¦¬í•˜ê¸°\n",
                "\n",
                "ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ê°€ ìˆ˜ë°± ê°œê°€ ë„˜ì–´ê°€ë©´ ì½”ë“œì— ë‹¤ ì ê¸° í˜ë“œë‹ˆê¹Œ,\n",
                "`stopwords.txt` ê°™ì€íŒŒì¼ì— ì €ì¥í•´ë‘ê³  ë¶ˆëŸ¬ì™€ì„œ ì“°ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤.\n",
                "\n",
                "```python\n",
                "# íŒŒì¼ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“œëŠ” ë²• (ì°¸ê³ ìš©)\n",
                "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
                "    stopwords = f.read().splitlines()\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
