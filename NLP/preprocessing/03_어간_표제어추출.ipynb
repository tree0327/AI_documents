{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "08a57014",
            "metadata": {},
            "source": [
                "# ğŸŒ³ 03. ì–´ê°„ ì¶”ì¶œ(Stemming) & í‘œì œì–´ ì¶”ì¶œ(Lemmatization): ë‹¨ì–´ì˜ ë¿Œë¦¬ ì°¾ê¸°\n",
                "\n",
                "## 1. ê°œìš”\n",
                "ì˜ì–´ ë‹¨ì–´ëŠ” ìƒí™©ì— ë”°ë¼ ëª¨ì–‘ì´ ê³„ì† ë°”ë€ë‹ˆë‹¤. \n",
                "`play`, `plays`, `played`, `playing`... ë‹¤ ê°™ì€ ëœ»ì¸ë° ì»´í“¨í„°ëŠ” ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ì£ .\n",
                "ì´ ë³€ì‹ í•œ ë‹¨ì–´ë“¤ì„ **ì›ë˜ í˜•íƒœ(ê¸°ë³¸í˜•)**ë¡œ ë˜ëŒë¦¬ëŠ” ê¸°ìˆ ì„ ë°°ì›ë‹ˆë‹¤.\n",
                "\n",
                "### ğŸ”‘ í•µì‹¬ ìš©ì–´ ì‚¬ì „\n",
                "| ìš©ì–´ | ëœ» | ë¹„ìœ  |\n",
                "|:---:|:---:|:---:|\n",
                "| **Stem (ì–´ê°„)** | ë‹¨ì–´ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë‹´ì€ ë¼ˆëŒ€ | ë‚˜ë¬´ì˜ ì¤„ê¸° |\n",
                "| **Lemma (í‘œì œì–´)** | ì‚¬ì „ì— ë“±ì¬ëœ ë‹¨ì–´ì˜ ê¸°ë³¸í˜• | ì‹ë¬¼ ë„ê°ì— ì íŒ ì§„ì§œ ì´ë¦„ |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c5141f6c",
            "metadata": {},
            "source": [
                "## 2. ì–´ê°„ ì¶”ì¶œ vs í‘œì œì–´ ì¶”ì¶œ, ë­ê°€ ë‹¤ë¥¸ê°€ìš”?\n",
                "\n",
                "> **ğŸŒ± ì‰¬ìš´ ë¹„ìœ **\n",
                ">\n",
                "> - **ì–´ê°„ ì¶”ì¶œ (Stemming)**: **\"ì¡ì´ˆ ê¹ëŠ” ê¸°ê³„\"**\n",
                ">   - ë¬´ì‹í•˜ê²Œ ê¸¸ì´ë§Œ ë§ì¶¥ë‹ˆë‹¤. `arguing` â¡ï¸ `argu` (ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´ê°€ ë  ìˆ˜ë„ ìˆìŒ)\n",
                ">   - ë¹ ë¥´ì§€ë§Œ ê±°ì¹©ë‹ˆë‹¤.\n",
                ">\n",
                "> - **í‘œì œì–´ ì¶”ì¶œ (Lemmatization)**: **\"ì „ë¬¸ ì •ì›ì‚¬\"**\n",
                ">   - ì‹ë¬¼ì˜ ì¢…ë¥˜ë¥¼ ë³´ê³  ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ìŠµë‹ˆë‹¤. `arguing` â¡ï¸ `argue` (ì§„ì§œ ë‹¨ì–´!)\n",
                ">   - ëŠë¦¬ì§€ë§Œ ì •í™•í•©ë‹ˆë‹¤.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3b5a13ce",
            "metadata": {},
            "source": [
                "## 3. ì–´ê°„ ì¶”ì¶œ (Stemming) ì‹¤ìŠµ\n",
                "\n",
                "**í¬í„° ìŠ¤í…Œë¨¸(Porter Stemmer)**ë¼ëŠ” ìœ ëª…í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì¨ë´…ì‹œë‹¤.\n",
                "ì˜ì–´ì˜ ì ‘ë¯¸ì‚¬(`-ing`, `-ed`, `-s` ë“±)ë¥¼ ë–¼ì–´ë‚´ëŠ” ê·œì¹™ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8341e201",
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.stem import PorterStemmer\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "# 1. ìŠ¤í…Œë¨¸(ê¸°ê³„) ìƒì„±\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "# ì˜ˆì‹œ ë¬¸ì¥\n",
                "text = \"The runners were running swiftly and easily. Then ran past the finish line.\"\n",
                "\n",
                "# 2. ë‹¨ì–´ë¡œ ìª¼ê°œê¸°\n",
                "tokens = word_tokenize(text)\n",
                "\n",
                "# 3. ì–´ê°„ ì¶”ì¶œ ì ìš©í•˜ê¸°\n",
                "# ê° ë‹¨ì–´(token)ë¥¼ ê¸°ê³„ì— ë„£ì–´(stem) ë´…ë‹ˆë‹¤.\n",
                "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
                "\n",
                "print(\"ì›ë³¸:\", tokens)\n",
                "print(\"ê²°ê³¼:\", stemmed_tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "278d1068",
            "metadata": {},
            "source": [
                "### ğŸ¤” ê²°ê³¼ ë¶„ì„ (ê¸°ê³„ì˜ ì‹¤ìˆ˜)\n",
                "1. `runners` â¡ï¸ `runner` (ì„±ê³µ!)\n",
                "2. `easily` â¡ï¸ `easili` (ì‹¤íŒ¨! `y`ë¥¼ `i`ë¡œ ë°”ê¾¸ëŠ” ë‹¨ìˆœ ê·œì¹™ ë•Œë¬¸)\n",
                "3. `ran` â¡ï¸ `ran` (ì‹¤íŒ¨! ë¶ˆê·œì¹™ ë™ì‚¬ `run`ì„ ëª¨ë¦„)\n",
                "\n",
                "ì´ì²˜ëŸ¼ ì–´ê°„ ì¶”ì¶œì€ ì™„ë²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e4b42d5c",
            "metadata": {},
            "source": [
                "## 4. í‘œì œì–´ ì¶”ì¶œ (Lemmatization) ì‹¤ìŠµ\n",
                "\n",
                "ì´ë²ˆì—” ë˜‘ë˜‘í•œ ì •ì›ì‚¬, **WordNetLemmatizer**ë¥¼ ì¨ë´…ì‹œë‹¤.\n",
                "ì´ ì¹œêµ¬ëŠ” **í’ˆì‚¬(ë™ì‚¬, ëª…ì‚¬ ë“±)**ë¥¼ ì•Œë ¤ì¤˜ì•¼ ì œëŒ€ë¡œ ì¼í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ee78ffe6",
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.stem import WordNetLemmatizer\n",
                "import nltk\n",
                "\n",
                "# í‘œì œì–´ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)\n",
                "nltk.download('wordnet')\n",
                "\n",
                "# ì •ì›ì‚¬ ìƒì„±\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# ê°€ì¥ ì–´ë ¤ìš´ 'ran'(runì˜ ê³¼ê±°í˜•)ì„ ì‹œì¼œë´…ì‹œë‹¤.\n",
                "\n",
                "# 1. ê·¸ëƒ¥ ì‹œì¼°ì„ ë•Œ (í’ˆì‚¬ë¥¼ ì•ˆ ì•Œë ¤ì¤Œ)\n",
                "print(\"ê·¸ëƒ¥:\", lemmatizer.lemmatize('ran')) \n",
                "# ê²°ê³¼: ran (ëª…ì‚¬ë¼ê³  ì°©ê°í•´ì„œ ê·¸ëŒ€ë¡œ ë‘ )\n",
                "\n",
                "# 2. ë™ì‚¬(verb)ë¼ê³  ì•Œë ¤ì¤¬ì„ ë•Œ ('v')\n",
                "print(\"ë™ì‚¬:\", lemmatizer.lemmatize('ran', pos='v'))\n",
                "# ê²°ê³¼: run (ì˜¤! ë˜‘ë˜‘í•˜ê²Œ ì›í˜•ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52a8d2bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ì•„ê¹Œ ì‹¤íŒ¨í–ˆë˜ ë¬¸ì¥ ë‹¤ì‹œ ë„ì „\n",
                "# ëª¨ë“  ë‹¨ì–´ë¥¼ 'ë™ì‚¬'ë¼ê³  ê°€ì •í•˜ê³  ëŒë ¤ë´…ë‹ˆë‹¤.\n",
                "lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
                "\n",
                "print(\"í‘œì œì–´(ë™ì‚¬ ê¸°ì¤€) ê²°ê³¼:\", lemmatized_tokens)\n",
                "# 'were' -> 'be', 'running' -> 'run' ìœ¼ë¡œ ì˜ ë°”ë€ë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "de158f23",
            "metadata": {},
            "source": [
                "## ë§ˆë¬´ë¦¬\n",
                "\n",
                "- ê²€ìƒ‰ ì—”ì§„(êµ¬ê¸€)ì²˜ëŸ¼ **ì†ë„**ê°€ ì¤‘ìš”í•˜ê³  ëŒ€ì¶© ë¹„ìŠ·í•˜ë©´ ëœë‹¤? â¡ï¸ **ì–´ê°„ ì¶”ì¶œ**\n",
                "- ì±—ë´‡ì²˜ëŸ¼ **ì •í™•í•œ ì˜ë¯¸**ì™€ ë¬¸ë²•ì´ ì¤‘ìš”í•˜ë‹¤? â¡ï¸ **í‘œì œì–´ ì¶”ì¶œ**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
