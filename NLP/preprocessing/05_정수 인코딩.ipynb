{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ”¢ 05. ì •ìˆ˜ ì¸ì½”ë”© (Integer Encoding): ì»´í“¨í„°ì—ê²Œ ìˆ«ì ì´ë¦„í‘œ ë‹¬ì•„ì£¼ê¸°\n",
                "\n",
                "## 1. ê°œìš”\n",
                "ì»´í“¨í„°ëŠ” 'ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜' ê°™ì€ ê¸€ìë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì˜¤ì§ **ìˆ«ì**ë§Œ ê³„ì‚°í•  ìˆ˜ ìˆì£ .\n",
                "ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ì— **ê³ ìœ í•œ ë²ˆí˜¸(ì¶œì„ ë²ˆí˜¸)**ë¥¼ ë¶™ì—¬ì¤„ ê²ƒì…ë‹ˆë‹¤.\n",
                "\n",
                "> **ì‰¬ìš´ ë¹„ìœ **:  \n",
                "> - êµì‹¤ì— í•™ìƒì´ 30ëª… ìˆìŠµë‹ˆë‹¤.\n",
                "> - ì„ ìƒë‹˜ì´ ë¶€ë¥¼ ë•Œ \"ê¹€ì² ìˆ˜!\"ë¼ê³  ë¶€ë¥¼ ìˆ˜ë„ ìˆì§€ë§Œ, í–‰ì • ì²˜ë¦¬ë¥¼ ìœ„í•´ **\"1ë²ˆ!\"** ì´ë¼ê³  ë¶€ë¥´ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì¼ ë•Œê°€ ë§ìŠµë‹ˆë‹¤.\n",
                "> - ì´ì²˜ëŸ¼ ë‹¨ì–´(í•™ìƒ)ì—ê²Œ ë²ˆí˜¸(ì¶œì„ ë²ˆí˜¸)ë¥¼ ë¶€ì—¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. ì‘ì—… ìˆœì„œ (ë ˆì‹œí”¼)\n",
                "\n",
                "1. **ë¬¸ì¥ ìª¼ê°œê¸° (í† í°í™”)**: ê¸€ì„ ë‹¨ì–´ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
                "2. **ë‹¨ì–´ ì •ë¦¬í•˜ê¸° (ë¹ˆë„ìˆ˜ ê³„ì‚°)**: ì–´ë–¤ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì…‰ë‹ˆë‹¤.\n",
                "3. **ë²ˆí˜¸ ë§¤ê¸°ê¸° (ì¸ë±ì‹±)**: \n",
                "    - ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ì¼ìˆ˜ë¡ **ì• ë²ˆí˜¸(1ë²ˆ, 2ë²ˆ...)**ë¥¼ ì¤ë‹ˆë‹¤.\n",
                "    - ì™œìš”? ìì£¼ ì“°ëŠ” ë‹¨ì–´ë¥¼ ì‘ì€ ìˆ«ìë¡œ ë‘ë©´ ë©”ëª¨ë¦¬ íš¨ìœ¨ì— ì¢‹ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
                "4. **ë³€í™˜í•˜ê¸° (ì¸ì½”ë”©)**: ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ ìˆ«ìë¡œ ì‹¹ ë°”ê¿‰ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì‹¤ìŠµ ë°ì´í„° ì¤€ë¹„\n",
                "ì–´ë¦° ì™•ì(The Little Prince) í…ìŠ¤íŠ¸ë¥¼ ì¼ë¶€ ê°€ì ¸ì™€ì„œ ì—°ìŠµí•´ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "raw_text = \"\"\"The Little Prince, written by Antoine de Saint-ExupÃ©ry, is a poetic tale about a young prince who travels from his home planet to Earth. The story begins with a pilot stranded in the Sahara Desert after his plane crashes. While trying to fix his plane, he meets a mysterious young boy, the Little Prince.\n",
                "\n",
                "The Little Prince comes from a small asteroid called B-612, where he lives alone with a rose that he loves deeply. He recounts his journey to the pilot, describing his visits to several other planets. Each planet is inhabited by a different character, such as a king, a vain man, a drunkard, a businessman, a geographer, and a fox. Through these encounters, the Prince learns valuable lessons about love, responsibility, and the nature of adult behavior.\n",
                "\n",
                "On Earth, the Little Prince meets various creatures, including a fox, who teaches him about relationships and the importance of taming, which means building ties with others. The fox's famous line, \"You become responsible, forever, for what you have tamed,\" resonates with the Prince's feelings for his rose.\n",
                "\n",
                "Ultimately, the Little Prince realizes that the essence of life is often invisible and can only be seen with the heart. After sharing his wisdom with the pilot, he prepares to return to his asteroid and his beloved rose. The story concludes with the pilot reflecting on the lessons learned from the Little Prince and the enduring impact of their friendship.\n",
                "\n",
                "The narrative is a beautifully simple yet profound exploration of love, loss, and the importance of seeing beyond the surface of things.\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ì „ì²˜ë¦¬ (ì²­ì†Œí•˜ê¸°)\n",
                "- ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ ìª¼ê°œê³  (`tokenize`)\n",
                "- ëŒ€ì†Œë¬¸ìë¥¼ í†µì¼í•˜ê³  (`lower`)\n",
                "- ì“¸ëª¨ì—†ëŠ” ë‹¨ì–´ë¥¼ ë²„ë¦½ë‹ˆë‹¤ (`stopword removal`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "# 0. í•„ìš”í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "\n",
                "# 1. ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¨¼ì € ë‚˜ëˆ„ê¸°\n",
                "sentences = sent_tokenize(raw_text)\n",
                "\n",
                "# 2. ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³  ì²­ì†Œí•˜ê¸°\n",
                "vocab = {}  # ë‹¨ì–´ì¥ (ë‹¨ì–´: ë“±ì¥íšŸìˆ˜)\n",
                "preprocessed_sentences = [] # ì²­ì†Œëœ ë¬¸ì¥ë“¤ì„ ëª¨ì„ ë¦¬ìŠ¤íŠ¸\n",
                "stop_words = set(stopwords.words('english'))\n",
                "\n",
                "for sentence in sentences:\n",
                "    # ì†Œë¬¸ì ë³€í™˜ + ë‹¨ì–´ í† í°í™”\n",
                "    tokens = word_tokenize(sentence.lower())\n",
                "    \n",
                "    cleaned_tokens = []\n",
                "    for token in tokens:\n",
                "        # ë¶ˆìš©ì–´(the, is ë“±) ë¹¼ê³ , ê¸¸ì´ 2ë³´ë‹¤ í° ë‹¨ì–´ë§Œ ë‚¨ê¸°ê¸°\n",
                "        if token not in stop_words and len(token) > 2:\n",
                "            cleaned_tokens.append(token)\n",
                "            \n",
                "            # ë‹¨ì–´ì¥(vocab)ì— íšŸìˆ˜ ê¸°ë¡\n",
                "            if token not in vocab:\n",
                "                vocab[token] = 0\n",
                "            vocab[token] += 1\n",
                "            \n",
                "    preprocessed_sentences.append(cleaned_tokens)\n",
                "\n",
                "print(\"ì²­ì†Œëœ ì²« ë¬¸ì¥:\", preprocessed_sentences[0])\n",
                "print(\"ë°œê²¬ëœ ì´ ë‹¨ì–´ ê°œìˆ˜:\", len(vocab))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. ë²ˆí˜¸í‘œ ë°°ë¶€ (ë¹ˆë„ìˆ˜ ì •ë ¬ & ì¸ë±ì‹±)\n",
                "ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë‹¨ì–´ë¶€í„° 1ë²ˆì„ ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. ë§ì´ ë‚˜ì˜¨ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)\n",
                "# vocab.items()ëŠ” (ë‹¨ì–´, íšŸìˆ˜) ìŒì„ ì¤ë‹ˆë‹¤. -> x[1]ì€ íšŸìˆ˜!\n",
                "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(\"ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹¨ì–´ Top 5:\", vocab_sorted[:5])\n",
                "\n",
                "# 2. ë²ˆí˜¸ ë§¤ê¸°ê¸°\n",
                "word_to_index = {}\n",
                "i = 0\n",
                "\n",
                "for (word, frequency) in vocab_sorted:\n",
                "    # ë¹ˆë„ìˆ˜ê°€ ë„ˆë¬´ ì ì€(1ë²ˆë§Œ ë‚˜ì˜¨) ë‹¨ì–´ëŠ” ì œì™¸í•´ë³¼ê¹Œìš”?\n",
                "    if frequency > 1:\n",
                "        i = i + 1\n",
                "        word_to_index[word] = i\n",
                "\n",
                "print(\"\\në²ˆí˜¸í‘œ ë°°ë¶€ ì™„ë£Œ! (ì¼ë¶€ë§Œ ì¶œë ¥)\")\n",
                "# ì•ì—ì„œ 5ê°œë§Œ ì¶œë ¥í•´ë³´ê¸°\n",
                "# ë”•ì…”ë„ˆë¦¬ëŠ” ìŠ¬ë¼ì´ì‹±ì´ ì•ˆ ë¼ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿”ì„œ ì¶œë ¥\n",
                "print(list(word_to_index.items())[:5])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### âš ï¸ OOV (Out-Of-Vocabulary): ëª¨ë¥´ëŠ” ë‹¨ì–´ ì²˜ë¦¬\n",
                "\n",
                "ë§Œì•½ ë‹¨ì–´ì¥(word_to_index)ì— ì—†ëŠ” ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ë‚˜ì˜¤ë©´ ì–´ë–»ê²Œ í• ê¹Œìš”?\n",
                "ê·¸ëŸ´ ë• **\"ê¸°íƒ€ ë“±ë“±\"**ì„ ì˜ë¯¸í•˜ëŠ” íŠ¹ë³„í•œ ë²ˆí˜¸ë¥¼ ì¤ë‹ˆë‹¤.\n",
                "ë³´í†µ `OOV` ë˜ëŠ” `UNK` (Unknown)ë¼ëŠ” ì´ë¦„ì„ ì“°ê³ , ê°€ì¥ ë§ˆì§€ë§‰ ë²ˆí˜¸ë¥¼ ì¤ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vocab_size = len(word_to_index)\n",
                "\n",
                "# ë‹¨ì–´ì¥ ë§¨ ë ë²ˆí˜¸ + 1ì„ OOV ë²ˆí˜¸ë¡œ ì§€ì •\n",
                "word_to_index['OOV'] = vocab_size + 1\n",
                "\n",
                "print(\"OOVì˜ ë²ˆí˜¸:\", word_to_index['OOV'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (Encoding)\n",
                "\n",
                "ì´ì œ ë‹¨ì–´ì¥ì„ ë³´ë©´ì„œ ë¬¸ì¥ì˜ ë‹¨ì–´ë“¤ì„ ë²ˆí˜¸ë¡œ ë°”ê¿”ì¹˜ê¸°í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoded_sentences = []\n",
                "\n",
                "for sentence in preprocessed_sentences:\n",
                "    encoded_sentence = []\n",
                "    for word in sentence:\n",
                "        try:\n",
                "            # ë‹¨ì–´ì¥ì— ìˆìœ¼ë©´ ê·¸ ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜´\n",
                "            encoded_sentence.append(word_to_index[word])\n",
                "        except KeyError:\n",
                "            # ë‹¨ì–´ì¥ì— ì—†ìœ¼ë©´ OOV ë²ˆí˜¸ë¥¼ ê°€ì ¸ì˜´\n",
                "            encoded_sentence.append(word_to_index['OOV'])\n",
                "            \n",
                "    encoded_sentences.append(encoded_sentence)\n",
                "\n",
                "print(\"ì›ë³¸:\", preprocessed_sentences[0])\n",
                "print(\"ë³€í™˜:\", encoded_sentences[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. ê¿€íŒ: Keras Tokenizer (ìë™í™” ë„êµ¬)\n",
                "\n",
                "ìœ„ ê³¼ì •ì„ íŒŒì´ì¬ forë¬¸ìœ¼ë¡œ ì§œë©´ ë³µì¡í•˜ì£ ? \n",
                "**Keras**ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì“°ë©´ ì´ ëª¨ë“  ê±¸ 2ì¤„ë¡œ ëë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì‹¤ë¬´ì—ì„  ì´ê±¸ ì”ë‹ˆë‹¤!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "\n",
                "# ë‹¨ìˆœ ì˜ˆì‹œ ë¬¸ì¥ë“¤\n",
                "sentences = [\n",
                "    'I love my dog',\n",
                "    'I love my cat',\n",
                "    'You love my dog!',\n",
                "    'Do you think my dog is amazing?'\n",
                "]\n",
                "\n",
                "# 1. í† í¬ë‚˜ì´ì € ìƒì„± (OOV í† í° ì´ë¦„ ì§€ì •)\n",
                "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
                "\n",
                "# 2. ë‹¨ì–´ì¥ ë§Œë“¤ê¸° (fit_on_texts)\n",
                "# ë¬¸ì¥ë“¤ì„ ì£¼ë©´ ì•Œì•„ì„œ ë¹ˆë„ìˆ˜ ê³„ì‚°í•˜ê³  ë²ˆí˜¸ ë§¤ê¹ë‹ˆë‹¤.\n",
                "tokenizer.fit_on_texts(sentences)\n",
                "\n",
                "# ë‹¨ì–´ì¥ í™•ì¸ (word_index)\n",
                "print(\"ë‹¨ì–´ì¥:\", tokenizer.word_index)\n",
                "\n",
                "# 3. ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° (texts_to_sequences)\n",
                "sequences = tokenizer.texts_to_sequences(sentences)\n",
                "\n",
                "print(\"\\në³€í™˜ ê²°ê³¼:\")\n",
                "for seq in sequences:\n",
                "    print(seq)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ë§ˆë¬´ë¦¬ ìš”ì•½\n",
                "\n",
                "1. ì»´í“¨í„°ë¥¼ ìœ„í•´ ê¸€ìì— **ë²ˆí˜¸í‘œ(Index)**ë¥¼ ë¶™ì…ë‹ˆë‹¤.\n",
                "2. **ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´**ì¼ìˆ˜ë¡ ì• ë²ˆí˜¸(1, 2...)ë¥¼ ì¤ë‹ˆë‹¤.\n",
                "3. ëª¨ë¥´ëŠ” ë‹¨ì–´ëŠ” **OOV**ë¼ëŠ” í‰ì¹˜ëŠ” ë²ˆí˜¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
                "4. ì‹¤ì „ì—ì„œëŠ” **Keras Tokenizer**ë¥¼ ì“°ë©´ ë§¤ìš° í¸í•©ë‹ˆë‹¤."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}