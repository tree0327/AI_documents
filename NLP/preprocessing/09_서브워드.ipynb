{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "3ebf8268",
            "metadata": {},
            "source": [
                "# ğŸ§© 09. ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € (Subword Tokenizer): ë‹¨ì–´ ìª¼ê°œê¸° ê¸°ìˆ \n",
                "\n",
                "## 1. ê°œìš”\n",
                "ì§€ê¸ˆê¹Œì§€ëŠ” ë‹¨ì–´ë¥¼ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ´ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¸ìƒì—” ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ê³„ì† ìƒê²¨ë‚©ë‹ˆë‹¤. (ì˜ˆ: `ChatGPT`, `JMT`...)\n",
                "ê¸°ê³„ê°€ ëª¨ë¥´ëŠ” ë‹¨ì–´(OOV)ë¥¼ ë§Œë‚˜ë©´ \"ëª¨ë¦„!\" í•˜ê³  í¬ê¸°í•´ë²„ë¦¬ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ë‹¨ì–´ë¥¼ ë” ì‘ì€ **ì˜ë¯¸ ì¡°ê°(Subword)**ìœ¼ë¡œ ìª¼ê°­ë‹ˆë‹¤.\n",
                "\n",
                "> **ì‰¬ìš´ ë¹„ìœ **:  \n",
                "> - `Unfriendly` ë¼ëŠ” ë‹¨ì–´ë¥¼ ëª¨ë¥¸ë‹¤ê³  ì¹©ì‹œë‹¤.\n",
                "> - í•˜ì§€ë§Œ `Un-` (ì•„ë‹Œ) + `friend` (ì¹œêµ¬) + `-ly` (í•˜ê²Œ) ë¡œ ìª¼ê°œì„œ ë³´ë©´ ëœ»ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n",
                "> - ì´ê²Œ ë°”ë¡œ **ì„œë¸Œì›Œë“œ(Subword)** ë°©ì‹ì…ë‹ˆë‹¤.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "39dfa98f",
            "metadata": {},
            "source": [
                "## 2. ì™œ ì„œë¸Œì›Œë“œì¸ê°€ìš”?\n",
                "\n",
                "| ë°©ì‹ | ì˜ˆì‹œ | ì¥ì  | ë‹¨ì  |\n",
                "|:---:|---|---|---|\n",
                "| **ë‹¨ì–´ ë‹¨ìœ„** | `apple` | ì˜ë¯¸ íŒŒì•… ì‰¬ì›€ | ëª¨ë¥´ëŠ” ë‹¨ì–´ ë‚˜ì˜¤ë©´ ëì¥ (OOV) |\n",
                "| **ê¸€ì ë‹¨ìœ„** | `a`, `p`, `p`, `l`, `e` | OOV ì—†ìŒ | ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ì§, ì˜ë¯¸ íŒŒì•… ì–´ë ¤ì›€ |\n",
                "| **ì„œë¸Œì›Œë“œ** | `app` + `##le` | **OOV í•´ê²° + ì ë‹¹í•œ ê¸¸ì´** | **(í˜„ì¬ ê°€ì¥ ë§ì´ ì“°ëŠ” ë°©ì‹)** |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e0740e16",
            "metadata": {},
            "source": [
                "## 3. ì‹¤ìŠµ ë°ì´í„° ì¤€ë¹„ (ë„¤ì´ë²„ ì˜í™” ë¦¬ë·°)\n",
                "\n",
                "í•œêµ­ì–´ ë°ì´í„°ë¡œ ì‹¤ìŠµí•˜ê¸° ìœ„í•´ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ë°ì´í„°(NSMC)ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd485e07",
            "metadata": {},
            "outputs": [],
            "source": [
                "import urllib.request\n",
                "import os\n",
                "import pandas as pd\n",
                "\n",
                "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
                "def download_file(url, save_name):\n",
                "    if not os.path.exists(save_name):\n",
                "        print(f\"{save_name} ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
                "        urllib.request.urlretrieve(url, save_name)\n",
                "    else:\n",
                "        print(f\"{save_name} ì´ë¯¸ ì¡´ì¬í•¨\")\n",
                "\n",
                "download_file('https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt', 'ratings_train.txt')\n",
                "\n",
                "# 2. ë°ì´í„° ì½ê¸°\n",
                "train_df = pd.read_csv('ratings_train.txt', sep='\\t')\n",
                "train_df = train_df.dropna(how='any') # ê²°ì¸¡ì¹˜ ì œê±°\n",
                "\n",
                "print(f\"ë¦¬ë·° ê°œìˆ˜: {len(train_df)}ê°œ\")\n",
                "train_df.head() # ìƒìœ„ 5ê°œ í™•ì¸"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9277e209",
            "metadata": {},
            "outputs": [],
            "source": [
                "# í•™ìŠµì„ ìœ„í•´ ë¦¬ë·° ë‚´ìš©ë§Œ ë”°ë¡œ 'txt' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
                "with open('naver_review.txt', 'w', encoding='utf-8') as f:\n",
                "    f.write('\\n'.join(train_df['document'].values))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f9ffdfd7",
            "metadata": {},
            "source": [
                "## 4. êµ¬ê¸€ì˜ ê±¸ì‘: SentencePiece (SPM)\n",
                "\n",
                "êµ¬ê¸€ì´ ë§Œë“  ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì €ì…ë‹ˆë‹¤. í•œêµ­ì–´ ì²˜ë¦¬ì— ì•„ì£¼ ê°•í•©ë‹ˆë‹¤.\n",
                "\n",
                "### íŠ¹ì§•\n",
                "- ê³µë°±ì„ `_` (ë°‘ì¤„) ë¬¸ìë¡œ ë°”ê¿”ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
                "- ì˜ˆ: `ì•ˆë…• í•˜ì„¸ìš”` â¡ï¸ `_ì•ˆë…•`, `_í•˜ì„¸ìš”`\n",
                "- ì´ë ‡ê²Œ í•˜ë©´ ë‚˜ì¤‘ì— ë¬¸ì¥ìœ¼ë¡œ ë³µêµ¬í•˜ê¸°ê°€ ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤. (`_`ë¥¼ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ë¨)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2958741b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sentencepiece as spm\n",
                "\n",
                "# 1. í•™ìŠµ ì‹œí‚¤ê¸° (Train)\n",
                "# --input: í•™ìŠµí•  íŒŒì¼\n",
                "# --model_prefix: ì €ì¥í•  ì´ë¦„\n",
                "# --vocab_size: ë‹¨ì–´ ì§‘í•© í¬ê¸° (5000ê°œë§Œ ì“°ê² ë‹¤)\n",
                "# --model_type: ì•Œê³ ë¦¬ì¦˜ (bpeê°€ ìœ ëª…í•¨)\n",
                "spm.SentencePieceTrainer.Train(\n",
                "    '--input=naver_review.txt --model_prefix=naver --vocab_size=5000 --model_type=bpe --max_sentence_length=9999'\n",
                ")\n",
                "\n",
                "print(\"í•™ìŠµ ì™„ë£Œ! naver.model, naver.vocab íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec795f1a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. ë¡œë“œí•˜ê¸°\n",
                "sp = spm.SentencePieceProcessor()\n",
                "sp.Load('naver.model')\n",
                "\n",
                "# 3. í† í°í™” í…ŒìŠ¤íŠ¸\n",
                "text = \"ì§„ì§œ ì •ë§ ì¬ë°ŒëŠ” ì˜í™”ì˜€ìŠµë‹ˆë‹¤ ê¿€ì¼\"\n",
                "tokens = sp.encode_as_pieces(text)\n",
                "ids = sp.encode_as_ids(text)\n",
                "\n",
                "print(\"í† í°:\", tokens)\n",
                "print(\"ID:\", ids)\n",
                "# ê²°ê³¼ ì˜ˆì‹œ: [' ì§„ì§œ', ' ì •ë§', ' ì¬ë¯¸', 'ìˆëŠ”', ...]\n",
                "# ì•ì— ë¶™ì€ '_' ê¸°í˜¸ì— ì£¼ëª©í•˜ì„¸ìš”! (ë„ì–´ì“°ê¸° ì •ë³´)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1eb3c42",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. ë³µì› í…ŒìŠ¤íŠ¸ (Decode)\n",
                "decoded_text = sp.decode_ids(ids)\n",
                "print(\"ë³µì›:\", decoded_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6c6ece8d",
            "metadata": {},
            "source": [
                "## 5. BERTì˜ ì‹¬ì¥: WordPiece Tokenizer\n",
                "\n",
                "í—ˆê¹…í˜ì´ìŠ¤(Hugging Face) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "BERT ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìœ ëª…í•©ë‹ˆë‹¤.\n",
                "\n",
                "### íŠ¹ì§•\n",
                "- ë‹¨ì–´ ì¤‘ê°„ì— ì´ì–´ì§€ëŠ” ì¡°ê°ì—ëŠ” `##`ì„ ë¶™ì…ë‹ˆë‹¤.\n",
                "- ì˜ˆ: `playing` â¡ï¸ `play`, `##ing` \n",
                "- \"##ingëŠ” í˜¼ì ì“°ì´ëŠ” ê²Œ ì•„ë‹ˆë¼ ì• ë‹¨ì–´ì— ë¶™ì–´ìˆëŠ” ê±°ì•¼\" ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f4842856",
            "metadata": {},
            "outputs": [],
            "source": [
                "from tokenizers import BertWordPieceTokenizer\n",
                "\n",
                "# 1. í† í¬ë‚˜ì´ì € ìƒì„±\n",
                "tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
                "\n",
                "# 2. í•™ìŠµ\n",
                "tokenizer.train(\n",
                "    files=['naver_review.txt'],\n",
                "    vocab_size=5000,\n",
                "    min_frequency=5, # ìµœì†Œ 5ë²ˆì€ ë‚˜ì™€ì•¼ ë‹¨ì–´ë¡œ ì¸ì •\n",
                "    wordpieces_prefix='##' # ì´ì–´ì§€ëŠ” ì¡°ê° í‘œì‹œ ê¸°í˜¸\n",
                ")\n",
                "\n",
                "# 3. í…ŒìŠ¤íŠ¸\n",
                "encoded = tokenizer.encode(\"ì§„ì§œ ì •ë§ ì¬ë°ŒëŠ” ì˜í™”ì˜€ìŠµë‹ˆë‹¤ ê¿€ì¼\")\n",
                "\n",
                "print(\"í† í°:\", encoded.tokens)\n",
                "# ê²°ê³¼ ì˜ˆì‹œ: ['ì§„ì§œ', 'ì •ë§', 'ì¬ë°Œ', '##ëŠ”', ...]\n",
                "# '##'ì´ ë¶™ì€ ê²ƒì€ ì ‘ë¯¸ì‚¬ë¼ëŠ” ëœ»!\n",
                "\n",
                "print(\"ID:\", encoded.ids)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5c51bdd8",
            "metadata": {},
            "source": [
                "## ë§ˆë¬´ë¦¬ ìš”ì•½\n",
                "\n",
                "| êµ¬ë¶„ | SentencePiece (êµ¬ê¸€) | WordPiece (BERT) |\n",
                "|:---:|---|---|\n",
                "| **ê³µë°± ì²˜ë¦¬** | `_` (ì‚¬ìš©ì í¸ì˜ì„± ì¢‹ìŒ) | ë³„ë„ ì²˜ë¦¬ ì—†ìŒ |\n",
                "| **ê¸°í˜¸** | ì• `_` | ë’¤ `##` |\n",
                "| **ì‚¬ìš©ì²˜** | í•œêµ­ì–´ ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ë²”ìš©ì  | BERT ê³„ì—´ ëª¨ë¸ |\n",
                "\n",
                "> **ê²°ë¡ **: ìš”ì¦˜ ë”¥ëŸ¬ë‹(LLM)ì€ ëŒ€ë¶€ë¶„ ì´ **ì„œë¸Œì›Œë“œ í† í°í™”** ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
