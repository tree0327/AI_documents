{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "29e35586",
            "metadata": {},
            "source": [
                "# âœ‚ï¸ 01. í† í°í™” (Tokenization): ë¬¸ì¥ ìª¼ê°œê¸° ê¸°ìˆ \n",
                "\n",
                "## 1. í† í°í™”ê°€ ë­”ê°€ìš”?\n",
                "ë ˆê³  ì„±ì„ ë§Œë“¤ê¸° ìœ„í•´ **ë ˆê³  ì¡°ê°**ì´ í•„ìš”í•˜ë“¯ì´, AIê°€ ê¸€ì„ ì´í•´í•˜ë ¤ë©´ ë¬¸ì¥ì„ **ì‘ì€ ì¡°ê°(í† í°)**ìœ¼ë¡œ ë‚˜ëˆ ì•¼ í•©ë‹ˆë‹¤.\n",
                "ì´ ê³¼ì •ì„ **í† í°í™”(Tokenization)**ë¼ê³  í•©ë‹ˆë‹¤.\n",
                "\n",
                "> **ì‰¬ìš´ ë¹„ìœ **:  \n",
                "> \"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤\" â¡ï¸ `ì•„ë²„ì§€`, `ê°€`, `ë°©`, `ì—`, `ë“¤ì–´ê°€ì‹ ë‹¤` \n",
                "> ì´ë ‡ê²Œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë„ì–´ì“°ê¸°ë¥¼ í•´ì£¼ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•´ìš”!\n",
                "\n",
                "### ğŸ”‘ í•µì‹¬ ìš©ì–´ ì‚¬ì „\n",
                "| ìš©ì–´ | ëœ» | ë¹„ìœ  |\n",
                "|:---:|:---:|:---:|\n",
                "| **Corpus (ì½”í¼ìŠ¤)** | AI ê³µë¶€ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ëª¨ìŒ (ë§ë­‰ì¹˜) | êµê³¼ì„œ ì „ê¶Œ |\n",
                "| **Token (í† í°)** | ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„ | ë ˆê³  ì¡°ê° 1ê°œ |\n",
                "| **Tokenizer (í† í¬ë‚˜ì´ì €)** | ë¬¸ì¥ì„ í† í°ìœ¼ë¡œ ì˜ë¼ì£¼ëŠ” ë„êµ¬ | ì¬ë£Œ ì†ì§ˆí•˜ëŠ” ì¹¼ |\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71d30e5f",
            "metadata": {},
            "source": [
                "## 2. ì¤€ë¹„ë¬¼ ì±™ê¸°ê¸° (ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”©)\n",
                "\n",
                "ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë„ì™€ì£¼ëŠ” ë„êµ¬ ìƒì **NLTK(Natural Language Toolkit)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "ì‚¬ìš©í•˜ê¸° ì „ì— **'í† í°í™” ê°€ì´ë“œë¶(punkt)'**ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "43cba6db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë‹¤ìš´ë¡œë“œ ì‹œì‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\n",
                        "ì¤€ë¹„ ì™„ë£Œ!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to /Users/gimdabin/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     /Users/gimdabin/nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk  # ìì—°ì–´ ì²˜ë¦¬ ë„êµ¬ NLTK ë¶ˆëŸ¬ì˜¤ê¸°\n",
                "\n",
                "# NLTKì—ëŠ” ì—¬ëŸ¬ ê¸°ëŠ¥ì´ ìˆëŠ”ë°, ê·¸ì¤‘ì—ì„œ 'í† í°í™”' ê¸°ëŠ¥ì„ ì“°ë ¤ë©´\n",
                "# ë¯¸ë¦¬ í•™ìŠµëœ ë°ì´í„°(ê°€ì´ë“œë¶)ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "\n",
                "print(\"ë‹¤ìš´ë¡œë“œ ì‹œì‘... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)\")\n",
                "nltk.download('punkt')          # ë¬¸ì¥/ë‹¨ì–´ ë‚˜ëˆ„ëŠ” ë²•ì´ ì íŒ ë°ì´í„°\n",
                "nltk.download('punkt_tab')      # (ì˜¤ë¥˜ ë°©ì§€ìš©) ì¶”ê°€ ë¦¬ì†ŒìŠ¤\n",
                "print(\"ì¤€ë¹„ ì™„ë£Œ!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2ecc279f",
            "metadata": {},
            "source": [
                "## 3. ë‹¨ì–´ í† í°í™” (Word Tokenization)\n",
                "\n",
                "ê°€ì¥ ê¸°ë³¸ì ì´ê³  ë§ì´ ì“°ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
                "ë¬¸ì¥ì„ **ë‹¨ì–´(Word)** ë‹¨ìœ„ë¡œ ìë¦…ë‹ˆë‹¤. \n",
                "\n",
                "- ì˜ì–´: ì£¼ë¡œ **ë„ì–´ì“°ê¸°** ê¸°ì¤€\n",
                "- ì˜ˆì‹œ: `Mac Book` â¡ï¸ `Mac`, `Book`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "37ddad38",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ì›ë³¸ ë¬¸ì¥: NLP is fascinating. It has many applications in real-world scenarios.\n",
                        "ë‹¨ì–´ í† í°í™” ê²°ê³¼: ['NLP', 'is', 'fascinating', '.', 'It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import word_tokenize  # ë‹¨ì–´ ìë¥´ëŠ” ë„êµ¬ ê°€ì ¸ì˜¤ê¸°\n",
                "\n",
                "# ì‹¤ìŠµí•  ë¬¸ì¥ (ì˜ì–´)\n",
                "text = \"NLP is fascinating. It has many applications in real-world scenarios.\"\n",
                "\n",
                "# ë¬¸ì¥ì„ ë‹¨ì–´ ì¡°ê°ë“¤ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.\n",
                "tokenized_words = word_tokenize(text)\n",
                "\n",
                "print(\"ì›ë³¸ ë¬¸ì¥:\", text)\n",
                "print(\"ë‹¨ì–´ í† í°í™” ê²°ê³¼:\", tokenized_words)\n",
                "# ê²°ê³¼ í•´ì„: ë§ˆì¹¨í‘œ(.)ë„ í•˜ë‚˜ì˜ ë‹¨ì–´ì²˜ëŸ¼ ë”°ë¡œ ë–¨ì–´ì ¸ ë‚˜ì˜¨ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd973257",
            "metadata": {},
            "source": [
                "## 4. ë¬¸ì¥ í† í°í™” (Sentence Tokenization)\n",
                "\n",
                "ê¸´ ê¸€(ë¬¸ë‹¨)ì„ **ë¬¸ì¥** ë‹¨ìœ„ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. \n",
                "ë‹¨ìˆœíˆ ë§ˆì¹¨í‘œ(.)ë§Œ ë³´ê³  ìë¥´ë©´ `Ph.D.` ê°™ì€ ë‹¨ì–´ ë•Œë¬¸ì— ì—‰ëš±í•˜ê²Œ ì˜ë¦´ ìˆ˜ ìˆëŠ”ë°, NLTKëŠ” ë˜‘ë˜‘í•˜ê²Œ ë¬¸ì¥ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "3b8987b9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë¶„ë¦¬ëœ ë¬¸ì¥ ê°œìˆ˜: 2\n",
                        "ë¬¸ì¥ 1: I have a Ph.D. in AI.\n",
                        "ë¬¸ì¥ 2: natural language processing is fun!\n"
                    ]
                }
            ],
            "source": [
                "from nltk.tokenize import sent_tokenize  # ë¬¸ì¥ ìë¥´ëŠ” ë„êµ¬\n",
                "\n",
                "# ë§ˆì¹¨í‘œê°€ ë§ì•„ì„œ í—·ê°ˆë¦¬ê¸° ì‰¬ìš´ ë¬¸ì¥ ì˜ˆì‹œ\n",
                "text_paragraph = \"I have a Ph.D. in AI. natural language processing is fun!\"\n",
                "\n",
                "# ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
                "sentences = sent_tokenize(text_paragraph)\n",
                "\n",
                "print(\"ë¶„ë¦¬ëœ ë¬¸ì¥ ê°œìˆ˜:\", len(sentences))\n",
                "for i, sent in enumerate(sentences):\n",
                "    print(f\"ë¬¸ì¥ {i+1}: {sent}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5941db91",
            "metadata": {},
            "source": [
                "### ğŸ”„ ì‘ìš©: ê¸´ ê¸€ì„ ë‹¨ì–´ë¡œ ìª¼ê°œê¸° (ë¬¸ì¥ ë¶„ë¦¬ â¡ï¸ ë‹¨ì–´ ë¶„ë¦¬)\n",
                "ì‹¤ë¬´ì—ì„œëŠ” ë³´í†µ ì´ë ‡ê²Œ í•©ë‹ˆë‹¤:\n",
                "1. ê¸´ ê¸€ì„ ë¬¸ì¥ë“¤ë¡œ ë‚˜ëˆˆë‹¤.\n",
                "2. ê° ë¬¸ì¥ì„ ë‹¤ì‹œ ë‹¨ì–´ë“¤ë¡œ ë‚˜ëˆˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "15d2f220",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "------------------\n",
                        "ë¬¸ì¥: I have a Ph.D. in AI.\n",
                        "ë‹¨ì–´ë“¤: ['I', 'have', 'a', 'Ph.D.', 'in', 'AI', '.']\n",
                        "------------------\n",
                        "ë¬¸ì¥: natural language processing is fun!\n",
                        "ë‹¨ì–´ë“¤: ['natural', 'language', 'processing', 'is', 'fun', '!']\n"
                    ]
                }
            ],
            "source": [
                "# ê° ë¬¸ì¥ì„ í•˜ë‚˜ì”© êº¼ë‚´ì„œ ë‹¨ì–´ í† í°í™”ë¥¼ ì§„í–‰í•´ë´…ë‹ˆë‹¤.\n",
                "for sent in sentences:\n",
                "    print(\"------------------\")\n",
                "    print(\"ë¬¸ì¥:\", sent)\n",
                "    print(\"ë‹¨ì–´ë“¤:\", word_tokenize(sent))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d2b688c",
            "metadata": {},
            "source": [
                "## 5. ìš”ì¦˜ ëŒ€ì„¸! ì„œë¸Œì›Œë“œ í† í°í™” (Subword Tokenization)\n",
                "\n",
                "ë‹¨ì–´ë¥¼ ë” ì‘ì€ **ì˜ë¯¸ ë‹¨ìœ„**ë¡œ ìª¼ê°œëŠ” íšê¸°ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. (BERT, GPT ê°™ì€ ìµœì‹  ëª¨ë¸ì€ ë‹¤ ì´ê±¸ ì”ë‹ˆë‹¤!)\n",
                "\n",
                "### ì™œ ìª¼ê°œë‚˜ìš”? (OOV ë¬¸ì œ í•´ê²°)\n",
                "- AIê°€ `apple`ì€ ë°°ì› ëŠ”ë°, `pineapple`ì„ ì²˜ìŒ ë´¤ë‹¤ê³  ì¹©ì‹œë‹¤.\n",
                "- ì˜›ë‚  ë°©ì‹: \"ëª¨ë¥´ëŠ” ë‹¨ì–´ë‹¤! ì—ëŸ¬!\" (OOV ë¬¸ì œ)\n",
                "- ì„œë¸Œì›Œë“œ ë°©ì‹: \"`pine` + `apple` ì´ë„¤? ì†Œë‚˜ë¬´ + ì‚¬ê³¼?\" (ì¶”ë¡  ê°€ëŠ¥!)\n",
                "\n",
                "### BERT í† í¬ë‚˜ì´ì € ì‹¤ìŠµ\n",
                "- `##`: \"ì´ ì¡°ê°ì€ ì• ë‹¨ì–´ì— ë¶™ì–´ìˆë˜ ê±°ì•¼\"ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "cf492bd8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ë‹¨ì–´: unhappiness\n",
                        "ë¶„í•´ ê²°ê³¼: ['un', '##ha', '##pp', '##iness']\n"
                    ]
                }
            ],
            "source": [
                "# í—ˆê¹…í˜ì´ìŠ¤(HuggingFace)ì˜ 'transformers' ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
                "from transformers import BertTokenizer\n",
                "\n",
                "# BERT ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°\n",
                "# 'uncased': ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì•ˆ í•¨ (ë‹¤ ì†Œë¬¸ìë¡œ ë°”ê¿ˆ)\n",
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "\n",
                "word = 'unhappiness'  # ë¶ˆí–‰ (un + happy + ness)\n",
                "\n",
                "# ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°\n",
                "subwords = tokenizer.tokenize(word)\n",
                "print(f\"ë‹¨ì–´: {word}\")\n",
                "print(f\"ë¶„í•´ ê²°ê³¼: {subwords}\")\n",
                "\n",
                "# ê²°ê³¼ í•´ì„: ['un', '##ha', '##pp', '##iness']\n",
                "# -> 'un'ì€ ì ‘ë‘ì‚¬ë¼ ë”°ë¡œ ë–¨ì–´ì§€ê³ , ë‚˜ë¨¸ì§€ëŠ” ì¡°ê°ì¡°ê° ë‚˜ë‰©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fc8fcefd",
            "metadata": {},
            "source": [
                "## 6. ë‚´ ë§˜ëŒ€ë¡œ ìë¥´ê¸°: ì •ê·œí‘œí˜„ì‹\n",
                "\n",
                "**\"ë‚˜ëŠ” íŠ¹ìˆ˜ë¬¸ìëŠ” ë‹¤ ë²„ë¦¬ê³  ì˜ì–´ ë‹¨ì–´ë§Œ ë‚¨ê¸°ê³  ì‹¶ì–´!\"** ì²˜ëŸ¼ íŠ¹ë³„í•œ ê·œì¹™ì´ í•„ìš”í•  ë•Œ ì”ë‹ˆë‹¤.\n",
                "`re` (Regular Expression) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "b9efc681",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['Time', 'files', 'like', 'an', 'arrow', 'fruit', 'files', 'like', 'a', 'banana']\n"
                    ]
                }
            ],
            "source": [
                "import re  # ì •ê·œí‘œí˜„ì‹ ë„êµ¬\n",
                "\n",
                "text = \"Time files like an arrow; fruit files like a banana.\"\n",
                "\n",
                "# ê·œì¹™: ì˜ì–´ ì•ŒíŒŒë²³ì´ 1ê°œ ì´ìƒ ì—°ì†ëœ ê²ƒë§Œ(íŠ¹ìˆ˜ë¬¸ì ì œì™¸) ì°¾ì•„ë¼!\n",
                "# \\w: ë¬¸ì(a-z, 0-9)\n",
                "# + : 1ê°œ ì´ìƒ\n",
                "regex_pattern = r\"\\w+\"\n",
                "\n",
                "tokens = re.findall(regex_pattern, text)\n",
                "print(tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "667949de",
            "metadata": {},
            "source": [
                "## ğŸ‡°ğŸ‡· 7. í•œêµ­ì–´ í† í°í™”: ëíŒì™• ë‚œì´ë„\n",
                "\n",
                "ì˜ì–´ëŠ” ë„ì–´ì“°ê¸°ë§Œ ì˜í•´ë„ ë°˜ì€ ê°€ëŠ”ë°, í•œêµ­ì–´ëŠ” ì–´ë µìŠµë‹ˆë‹¤.  \n",
                "**\"ë‚˜ëŠ”\"** = `ë‚˜` + `ëŠ”` (ëª…ì‚¬ + ì¡°ì‚¬)  \n",
                "ì´ë ‡ê²Œ ì°°ë‚˜ì˜ ë„ì–´ì“°ê¸° ì—†ì´ ë¶™ì–´ìˆëŠ” ë§ë“¤(**êµì°©ì–´**)ì„ ë–¼ì–´ë‚´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
                "\n",
                "### ë§ì´ ì“°ëŠ” ë„êµ¬: KoNLPy (ì½”ë„¬íŒŒì´)\n",
                "- í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë“¤ì˜ ëª¨ìŒì§‘ì…ë‹ˆë‹¤.\n",
                "- `Okt` (Open Korean Text): ì‚¬ìš©í•˜ê¸° ì‰½ê³  ë¹¨ë¼ì„œ ì…ë¬¸ìš©ìœ¼ë¡œ ì¢‹ìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "068fdeec",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "í˜•íƒœì†Œ ë¶„ë¦¬: ['ì•„ë²„ì§€', 'ê°€ë°©', 'ì—', 'ë“¤ì–´ê°€ì‹ ë‹¤']\n",
                        "ëª…ì‚¬ë§Œ ì¶”ì¶œ: ['ì•„ë²„ì§€', 'ê°€ë°©']\n",
                        "í’ˆì‚¬ íƒœê¹…: [('ì•„ë²„ì§€', 'Noun'), ('ê°€ë°©', 'Noun'), ('ì—', 'Josa'), ('ë“¤ì–´ê°€ì‹ ë‹¤', 'Verb')]\n"
                    ]
                }
            ],
            "source": [
                "from konlpy.tag import Okt  # Okt ë¶„ì„ê¸° ê°€ì ¸ì˜¤ê¸°\n",
                "\n",
                "okt = Okt()  # ë¡œë´‡ ìƒì„±!\n",
                "\n",
                "korean_text = \"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤\"\n",
                "\n",
                "# 1. í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (morphs)\n",
                "morphs = okt.morphs(korean_text)\n",
                "print(\"í˜•íƒœì†Œ ë¶„ë¦¬:\", morphs)\n",
                "\n",
                "# 2. ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ê¸° (nouns)\n",
                "nouns = okt.nouns(korean_text)\n",
                "print(\"ëª…ì‚¬ë§Œ ì¶”ì¶œ:\", nouns)\n",
                "\n",
                "# 3. í’ˆì‚¬ê¹Œì§€ ì•Œë ¤ì£¼ê¸° (pos)\n",
                "pos = okt.pos(korean_text)\n",
                "print(\"í’ˆì‚¬ íƒœê¹…:\", pos)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc3dbac5",
            "metadata": {},
            "source": [
                "### í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬ê¸° (KSS)\n",
                "\n",
                "í•œêµ­ì–´ëŠ” \".\"ì´ ì—†ì–´ë„ ë¬¸ì¥ì´ ëë‚˜ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì „ìš© ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "`kss` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì•„ì£¼ ìœ ëª…í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a87a6bc0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
                        "For your information, Kss also supports mecab backend.\n",
                        "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
                        "Please refer to following web sites for details:\n",
                        "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
                        "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
                        "\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['ë°¥ ë¨¹ì—ˆì–´?', 'ë‚˜ë‘ ë†€ì ì§‘ì— ê°€ì§€ë§ˆ']\n"
                    ]
                }
            ],
            "source": [
                "import kss\n",
                "\n",
                "text = \"ë°¥ ë¨¹ì—ˆì–´? ë‚˜ë‘ ë†€ì ì§‘ì— ê°€ì§€ë§ˆ\"\n",
                "\n",
                "# ë¬¸ë§¥ì„ íŒŒì•…í•´ì„œ ë¬¸ì¥ì„ ë‚˜ëˆ ì¤ë‹ˆë‹¤.\n",
                "split_sentences = kss.split_sentences(text)\n",
                "print(split_sentences)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nlp_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
