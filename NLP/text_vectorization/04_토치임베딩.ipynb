{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dbb498",
   "metadata": {},
   "source": [
    "# torch nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e148848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gimdabin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/gimdabin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gimdabin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67d1bb6",
   "metadata": {},
   "source": [
    "## 사전학습된 임베딩을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00bdfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [          \n",
    "    'nice great best amazing',  # 긍정 문장 예시\n",
    "    'stop lies',                # 부정/비판 문장 예시\n",
    "    'pitiful nerd',             # 부정 문장 예시\n",
    "    'excellent work',           # 긍정 문장 예시\n",
    "    'supreme quality',          # 긍정 문장 예시\n",
    "    'bad',                      # 부정 문장 예시\n",
    "    'highly respectable'        # 긍정 문장 예시\n",
    "]                               # 분류 모델에 넣을 입력 문장 리스트(list[str])\n",
    "labels = [1, 0, 0, 1, 1, 0, 1]  # 각 문장에 대한 이진 라벨(1=긍정, 0=부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34ee1043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nice', 'great', 'best', 'amazing'],\n",
       " ['stop', 'lies'],\n",
       " ['pitiful', 'nerd'],\n",
       " ['excellent', 'work'],\n",
       " ['supreme', 'quality'],\n",
       " ['bad'],\n",
       " ['highly', 'respectable']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sent) for sent in sentences] # 각 문장을 토큰 리스트(list(list[str]))로 변환\n",
    "tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a617b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'nice': 1, 'great': 1, 'best': 1, 'amazing': 1, 'stop': 1, 'lies': 1, 'pitiful': 1, 'nerd': 1, 'excellent': 1, 'work': 1, 'supreme': 1, 'quality': 1, 'bad': 1, 'highly': 1, 'respectable': 1})\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 생성 + 정수 인코딩\n",
    "from collections import Counter\n",
    "\n",
    "tokens = [token for sent in tokenized_sentences for token in sent] # 전체 문장 리스트를 평탄화하여 전체 토큰 리스트 생성\n",
    "word_counts = Counter(tokens) # 전체 토큰 등장 빈도 계산\n",
    "print(word_counts) # 토큰별 빈도 딕셔너리 형태 \n",
    "\n",
    "word_to_index = {word : index + 2 for index, word in enumerate(tokens)} # 토큰을 순서대로 인덱싱(+2 : 특수 토큰용)\n",
    "word_to_index['<PAD>'] = 0 # 패딩 토큰 (길이 맞추기용)\n",
    "word_to_index['<UNK>'] = 1 # OOV 토큰 (처리 불가 단어 대체)\n",
    "word_to_index = dict(sorted(word_to_index.items(), key=lambda x: x[1])) # 인덱스를 기준으로 정렬\n",
    "print(word_to_index) # 단어 -> 인덱스 사전\n",
    "\n",
    "vocab_size = len(word_to_index) # 전체 어휘 수 (특수토큰 포함)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ea664b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 인코딩 함수 : 토큰화된 문장 리스트를 단어 -> 인덱스 사전으로 정수 시퀀스 (list[list(int)])로 변환\n",
    "def texts_to_sequences(sentences, word_to_index):\n",
    "    sequences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        sequence = []\n",
    "\n",
    "        for token in sent:\n",
    "            if token in word_to_index:\n",
    "                sequence.append(word_to_index[token])   # 해당 단어 인덱스 추가\n",
    "            else:\n",
    "                sequence.append(word_to_index['<UNK>']) # 사전에 없으면 UNK 토큰\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd59375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5],\n",
       "       [ 6,  7,  0,  0],\n",
       "       [ 8,  9,  0,  0],\n",
       "       [10, 11,  0,  0],\n",
       "       [12, 13,  0,  0],\n",
       "       [14,  0,  0,  0],\n",
       "       [15, 16,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 서로 다른 길이의 정수 시퀀스를 0(<PAD>)으로 채워 (문장수, maxlen) 형태에 맞춰주는 함수\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    padded_sequences = np.zeros((len(sequences), maxlen), dtype=int) # (문장수 x maxlen) 크기의 0 패딩 배열\n",
    "    for index, seq in enumerate(sequences): # 각 문장 시퀀스 순회\n",
    "        padded_sequences[index, :len(seq)] = seq[:maxlen] # 앞에서부터 시퀀스 채운다. 길면 maxlen까지만 채워 자른다.\n",
    "\n",
    "    return padded_sequences # 패딩 작업 완료된 2D 배열\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen = 4) # 모든 문장 길이 4로 패딩/자르기\n",
    "padded_sequences # (문장 수, 4) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bb1fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6ac4b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (embedding): Embedding(17, 300, padding_idx=0)\n",
      "  (rnn): RNN(300, 16, batch_first=True)\n",
      "  (out): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Pytorch 텍스트 분류 모델 : Embedding + RNN + Linear로 이진분류(logit) 출력\n",
    "import torch\n",
    "import torch.nn as nn         # 신경망 레이어\n",
    "import torch.optim as optim   # 옵티마이저 (활성화함수)\n",
    "from torch.utils.data import DataLoader, TensorDataset # 배치로더 / 데이터셋 유틸\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    # 정수 시퀀스를 임베딩 -> RNN -> 선형층으로 처리해 이진 분류 logit(1개)를 출력\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                 # nn.Module 초기화\n",
    "        self.embedding = nn.Embedding(     # 단어 ID를 밀집 벡터로 변환하는 임베딩 층\n",
    "            num_embeddings = vocab_size,   # 단어 사전 크기 (어휘 수)\n",
    "            embedding_dim = embedding_dim, # 임베딩 차원\n",
    "            padding_idx = 0                # PAD(0) 인덱스는 0 그대로 사용\n",
    "        )\n",
    "\n",
    "        # 사전학습된 임베딩 벡터로 초기화 : Embedding 가중치를 사전학습 행렬로 덮어쓰기\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            embedding_dim,                 # 입력 차원\n",
    "            hidden_size,                   # 은닉 상태 차원\n",
    "            batch_first = True             # 배치 차원이 첫번째\n",
    "        )\n",
    "        self.out = nn.Linear(\n",
    "            hidden_size, 1                 # 마지막 은닉 상태를 1차원 logit으로 변환\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)       # (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        out, h_n = self.rnn(embedded)      # h_n : (num_layers * directions , batch, hidden_size)\n",
    "        out = self.out(h_n.squeeze(0))     # (batch_size, hidden_size) -> (batch, 1)\n",
    "        return out                         # 출력 : 시그모이드 전 logit(확률이 아님)\n",
    "\n",
    "embedding_dim = model_wv.vectors.shape[1] # 사전학습 임베딩 차원 (300)으로 임베딩 차원 설정\n",
    "model = SimpleNet(vocab_size, embedding_dim, hidden_size = 16) #  어휘 크기 / 임베딩 차원 / 은닉크기로 모델 생성\n",
    "print(model)\n",
    "criterion = nn.BCEWithLogitsLoss() # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb692c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "SimpleNet                                --\n",
       "├─Embedding: 1-1                         1,700\n",
       "├─RNN: 1-2                               1,888\n",
       "├─Linear: 1-3                            17\n",
       "=================================================================\n",
       "Total params: 3,605\n",
       "Trainable params: 3,605\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary # 모델 구조를 표 형태로 요약\n",
    "summary(model)                # 모델 구조 요약 표 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f78c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 100])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.153281</td>\n",
       "      <td>-0.282694</td>\n",
       "      <td>-0.317148</td>\n",
       "      <td>0.345025</td>\n",
       "      <td>-0.276356</td>\n",
       "      <td>-0.461011</td>\n",
       "      <td>0.374884</td>\n",
       "      <td>1.135668</td>\n",
       "      <td>-1.381616</td>\n",
       "      <td>0.289394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390174</td>\n",
       "      <td>-0.566019</td>\n",
       "      <td>0.088611</td>\n",
       "      <td>0.441200</td>\n",
       "      <td>-0.260525</td>\n",
       "      <td>0.381818</td>\n",
       "      <td>-0.467972</td>\n",
       "      <td>-1.061370</td>\n",
       "      <td>-1.269071</td>\n",
       "      <td>0.095704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.599027</td>\n",
       "      <td>0.703861</td>\n",
       "      <td>2.597956</td>\n",
       "      <td>-0.311437</td>\n",
       "      <td>-0.374255</td>\n",
       "      <td>0.882530</td>\n",
       "      <td>1.383746</td>\n",
       "      <td>-1.220286</td>\n",
       "      <td>-1.161161</td>\n",
       "      <td>0.814155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.045270</td>\n",
       "      <td>-0.994324</td>\n",
       "      <td>0.388893</td>\n",
       "      <td>0.837065</td>\n",
       "      <td>0.044579</td>\n",
       "      <td>1.669993</td>\n",
       "      <td>-0.244355</td>\n",
       "      <td>0.931893</td>\n",
       "      <td>0.286151</td>\n",
       "      <td>-0.758150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>1.736880</td>\n",
       "      <td>-0.418452</td>\n",
       "      <td>-3.528854</td>\n",
       "      <td>-0.184640</td>\n",
       "      <td>-0.411441</td>\n",
       "      <td>0.266664</td>\n",
       "      <td>0.264176</td>\n",
       "      <td>0.266506</td>\n",
       "      <td>0.469768</td>\n",
       "      <td>1.352606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387775</td>\n",
       "      <td>-0.350267</td>\n",
       "      <td>0.464201</td>\n",
       "      <td>1.230113</td>\n",
       "      <td>-0.046174</td>\n",
       "      <td>1.446777</td>\n",
       "      <td>0.633704</td>\n",
       "      <td>-1.435719</td>\n",
       "      <td>1.304379</td>\n",
       "      <td>-0.391658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.763722</td>\n",
       "      <td>0.654360</td>\n",
       "      <td>-0.604668</td>\n",
       "      <td>0.201209</td>\n",
       "      <td>1.285650</td>\n",
       "      <td>0.288810</td>\n",
       "      <td>-0.686289</td>\n",
       "      <td>-0.335848</td>\n",
       "      <td>1.287939</td>\n",
       "      <td>-0.911737</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.699164</td>\n",
       "      <td>0.296954</td>\n",
       "      <td>0.007868</td>\n",
       "      <td>0.251197</td>\n",
       "      <td>-1.955678</td>\n",
       "      <td>-0.176787</td>\n",
       "      <td>1.074953</td>\n",
       "      <td>0.650702</td>\n",
       "      <td>-0.161425</td>\n",
       "      <td>1.066105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>-0.602960</td>\n",
       "      <td>-0.163229</td>\n",
       "      <td>-1.363832</td>\n",
       "      <td>0.477234</td>\n",
       "      <td>-0.316091</td>\n",
       "      <td>0.819109</td>\n",
       "      <td>-0.153093</td>\n",
       "      <td>-0.615459</td>\n",
       "      <td>-1.237506</td>\n",
       "      <td>1.216410</td>\n",
       "      <td>...</td>\n",
       "      <td>2.597566</td>\n",
       "      <td>0.585246</td>\n",
       "      <td>-0.751663</td>\n",
       "      <td>-0.303269</td>\n",
       "      <td>-1.653833</td>\n",
       "      <td>0.942226</td>\n",
       "      <td>0.787624</td>\n",
       "      <td>-0.576481</td>\n",
       "      <td>0.725744</td>\n",
       "      <td>0.394482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.566338</td>\n",
       "      <td>-1.678163</td>\n",
       "      <td>0.332679</td>\n",
       "      <td>-0.019737</td>\n",
       "      <td>-1.472702</td>\n",
       "      <td>-0.223251</td>\n",
       "      <td>-1.543875</td>\n",
       "      <td>-0.780934</td>\n",
       "      <td>0.343145</td>\n",
       "      <td>-1.082000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.588946</td>\n",
       "      <td>0.692529</td>\n",
       "      <td>0.842667</td>\n",
       "      <td>-0.398742</td>\n",
       "      <td>0.830125</td>\n",
       "      <td>0.797854</td>\n",
       "      <td>0.109946</td>\n",
       "      <td>-0.614411</td>\n",
       "      <td>0.230818</td>\n",
       "      <td>0.812125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.809533</td>\n",
       "      <td>0.522360</td>\n",
       "      <td>-0.465712</td>\n",
       "      <td>-0.524149</td>\n",
       "      <td>0.249454</td>\n",
       "      <td>-1.220999</td>\n",
       "      <td>0.721737</td>\n",
       "      <td>-0.554861</td>\n",
       "      <td>-0.959020</td>\n",
       "      <td>0.421160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821864</td>\n",
       "      <td>0.798854</td>\n",
       "      <td>-1.243526</td>\n",
       "      <td>-0.884184</td>\n",
       "      <td>-0.175324</td>\n",
       "      <td>0.896553</td>\n",
       "      <td>-0.575425</td>\n",
       "      <td>1.444391</td>\n",
       "      <td>-0.954079</td>\n",
       "      <td>-0.310491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>-0.750028</td>\n",
       "      <td>-0.659472</td>\n",
       "      <td>1.905698</td>\n",
       "      <td>-1.021592</td>\n",
       "      <td>-0.307077</td>\n",
       "      <td>-1.790363</td>\n",
       "      <td>-0.952173</td>\n",
       "      <td>0.077647</td>\n",
       "      <td>0.855779</td>\n",
       "      <td>-0.102158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.587593</td>\n",
       "      <td>-0.533658</td>\n",
       "      <td>-0.845376</td>\n",
       "      <td>0.134653</td>\n",
       "      <td>-1.326125</td>\n",
       "      <td>-0.594806</td>\n",
       "      <td>-0.656765</td>\n",
       "      <td>-0.542460</td>\n",
       "      <td>-0.353943</td>\n",
       "      <td>0.272664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>-0.544908</td>\n",
       "      <td>0.410816</td>\n",
       "      <td>0.719263</td>\n",
       "      <td>-2.714604</td>\n",
       "      <td>-0.333282</td>\n",
       "      <td>-0.091262</td>\n",
       "      <td>1.829300</td>\n",
       "      <td>0.226831</td>\n",
       "      <td>-1.551552</td>\n",
       "      <td>-0.882857</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873997</td>\n",
       "      <td>-1.853501</td>\n",
       "      <td>1.114048</td>\n",
       "      <td>0.419245</td>\n",
       "      <td>-0.006181</td>\n",
       "      <td>0.306982</td>\n",
       "      <td>0.304988</td>\n",
       "      <td>-0.246282</td>\n",
       "      <td>-0.568133</td>\n",
       "      <td>-1.888943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-1.047282</td>\n",
       "      <td>-0.781588</td>\n",
       "      <td>0.357086</td>\n",
       "      <td>0.899275</td>\n",
       "      <td>-0.303635</td>\n",
       "      <td>1.411267</td>\n",
       "      <td>0.616851</td>\n",
       "      <td>0.189532</td>\n",
       "      <td>-0.538179</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225492</td>\n",
       "      <td>1.406254</td>\n",
       "      <td>2.085147</td>\n",
       "      <td>1.169223</td>\n",
       "      <td>-0.466419</td>\n",
       "      <td>-0.152135</td>\n",
       "      <td>-0.549252</td>\n",
       "      <td>-1.613023</td>\n",
       "      <td>0.022767</td>\n",
       "      <td>0.907434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.499717</td>\n",
       "      <td>-0.028547</td>\n",
       "      <td>0.366420</td>\n",
       "      <td>-1.650974</td>\n",
       "      <td>0.301834</td>\n",
       "      <td>-1.471861</td>\n",
       "      <td>-0.788189</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-0.789650</td>\n",
       "      <td>0.294089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.453951</td>\n",
       "      <td>-0.325911</td>\n",
       "      <td>1.647364</td>\n",
       "      <td>-1.082706</td>\n",
       "      <td>2.321183</td>\n",
       "      <td>-0.222166</td>\n",
       "      <td>-0.478727</td>\n",
       "      <td>2.313901</td>\n",
       "      <td>0.836022</td>\n",
       "      <td>-0.296454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>-0.148701</td>\n",
       "      <td>-0.317977</td>\n",
       "      <td>1.356056</td>\n",
       "      <td>-0.686991</td>\n",
       "      <td>-0.137686</td>\n",
       "      <td>-0.685240</td>\n",
       "      <td>-0.054415</td>\n",
       "      <td>-0.360146</td>\n",
       "      <td>-1.153097</td>\n",
       "      <td>-0.098618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046144</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>-0.542316</td>\n",
       "      <td>0.904640</td>\n",
       "      <td>1.039306</td>\n",
       "      <td>0.831886</td>\n",
       "      <td>0.178315</td>\n",
       "      <td>0.136362</td>\n",
       "      <td>-1.032011</td>\n",
       "      <td>0.149197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>0.433937</td>\n",
       "      <td>0.423834</td>\n",
       "      <td>-0.283111</td>\n",
       "      <td>-2.314933</td>\n",
       "      <td>-0.718427</td>\n",
       "      <td>-1.001299</td>\n",
       "      <td>1.983614</td>\n",
       "      <td>-0.029196</td>\n",
       "      <td>0.027408</td>\n",
       "      <td>0.168808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.967005</td>\n",
       "      <td>0.520769</td>\n",
       "      <td>1.299515</td>\n",
       "      <td>0.684447</td>\n",
       "      <td>0.284661</td>\n",
       "      <td>0.531108</td>\n",
       "      <td>-0.145330</td>\n",
       "      <td>1.392461</td>\n",
       "      <td>-0.272716</td>\n",
       "      <td>0.408863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-1.783883</td>\n",
       "      <td>-0.722886</td>\n",
       "      <td>1.179719</td>\n",
       "      <td>-1.235462</td>\n",
       "      <td>-1.101522</td>\n",
       "      <td>1.229072</td>\n",
       "      <td>-0.447362</td>\n",
       "      <td>-1.650954</td>\n",
       "      <td>-0.897583</td>\n",
       "      <td>-0.431812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.898886</td>\n",
       "      <td>-0.362620</td>\n",
       "      <td>-1.659595</td>\n",
       "      <td>-0.767814</td>\n",
       "      <td>2.082811</td>\n",
       "      <td>-0.417833</td>\n",
       "      <td>1.285412</td>\n",
       "      <td>0.265671</td>\n",
       "      <td>-1.730177</td>\n",
       "      <td>0.810932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>-1.495176</td>\n",
       "      <td>0.851895</td>\n",
       "      <td>0.407090</td>\n",
       "      <td>0.060314</td>\n",
       "      <td>0.683749</td>\n",
       "      <td>0.459398</td>\n",
       "      <td>-0.621154</td>\n",
       "      <td>-1.787921</td>\n",
       "      <td>0.251101</td>\n",
       "      <td>-1.471289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381117</td>\n",
       "      <td>0.958681</td>\n",
       "      <td>0.733722</td>\n",
       "      <td>-2.500595</td>\n",
       "      <td>0.071139</td>\n",
       "      <td>-0.161663</td>\n",
       "      <td>0.603788</td>\n",
       "      <td>1.312428</td>\n",
       "      <td>0.555324</td>\n",
       "      <td>-0.720070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.581376</td>\n",
       "      <td>-0.512886</td>\n",
       "      <td>-0.995378</td>\n",
       "      <td>0.807849</td>\n",
       "      <td>-1.919964</td>\n",
       "      <td>0.505726</td>\n",
       "      <td>0.131323</td>\n",
       "      <td>-0.615955</td>\n",
       "      <td>0.367982</td>\n",
       "      <td>1.046923</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.533116</td>\n",
       "      <td>1.449502</td>\n",
       "      <td>0.707508</td>\n",
       "      <td>-0.164226</td>\n",
       "      <td>1.165867</td>\n",
       "      <td>1.318780</td>\n",
       "      <td>0.545781</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>2.065585</td>\n",
       "      <td>-0.214659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.153281 -0.282694 -0.317148  0.345025 -0.276356 -0.461011   \n",
       "nice         0.599027  0.703861  2.597956 -0.311437 -0.374255  0.882530   \n",
       "great        1.736880 -0.418452 -3.528854 -0.184640 -0.411441  0.266664   \n",
       "best        -0.763722  0.654360 -0.604668  0.201209  1.285650  0.288810   \n",
       "amazing     -0.602960 -0.163229 -1.363832  0.477234 -0.316091  0.819109   \n",
       "stop        -0.566338 -1.678163  0.332679 -0.019737 -1.472702 -0.223251   \n",
       "lies         0.809533  0.522360 -0.465712 -0.524149  0.249454 -1.220999   \n",
       "pitiful     -0.750028 -0.659472  1.905698 -1.021592 -0.307077 -1.790363   \n",
       "nerd        -0.544908  0.410816  0.719263 -2.714604 -0.333282 -0.091262   \n",
       "excellent   -1.047282 -0.781588  0.357086  0.899275 -0.303635  1.411267   \n",
       "work         0.499717 -0.028547  0.366420 -1.650974  0.301834 -1.471861   \n",
       "supreme     -0.148701 -0.317977  1.356056 -0.686991 -0.137686 -0.685240   \n",
       "quality      0.433937  0.423834 -0.283111 -2.314933 -0.718427 -1.001299   \n",
       "bad         -1.783883 -0.722886  1.179719 -1.235462 -1.101522  1.229072   \n",
       "highly      -1.495176  0.851895  0.407090  0.060314  0.683749  0.459398   \n",
       "respectable  0.581376 -0.512886 -0.995378  0.807849 -1.919964  0.505726   \n",
       "\n",
       "                   6         7         8         9   ...        90        91  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.374884  1.135668 -1.381616  0.289394  ... -0.390174 -0.566019   \n",
       "nice         1.383746 -1.220286 -1.161161  0.814155  ...  2.045270 -0.994324   \n",
       "great        0.264176  0.266506  0.469768  1.352606  ... -0.387775 -0.350267   \n",
       "best        -0.686289 -0.335848  1.287939 -0.911737  ... -2.699164  0.296954   \n",
       "amazing     -0.153093 -0.615459 -1.237506  1.216410  ...  2.597566  0.585246   \n",
       "stop        -1.543875 -0.780934  0.343145 -1.082000  ...  1.588946  0.692529   \n",
       "lies         0.721737 -0.554861 -0.959020  0.421160  ... -0.821864  0.798854   \n",
       "pitiful     -0.952173  0.077647  0.855779 -0.102158  ... -0.587593 -0.533658   \n",
       "nerd         1.829300  0.226831 -1.551552 -0.882857  ...  1.873997 -1.853501   \n",
       "excellent    0.616851  0.189532 -0.538179  0.512987  ...  0.225492  1.406254   \n",
       "work        -0.788189  0.451719 -0.789650  0.294089  ...  1.453951 -0.325911   \n",
       "supreme     -0.054415 -0.360146 -1.153097 -0.098618  ...  0.046144  0.002158   \n",
       "quality      1.983614 -0.029196  0.027408  0.168808  ... -0.967005  0.520769   \n",
       "bad         -0.447362 -1.650954 -0.897583 -0.431812  ...  0.898886 -0.362620   \n",
       "highly      -0.621154 -1.787921  0.251101 -1.471289  ...  0.381117  0.958681   \n",
       "respectable  0.131323 -0.615955  0.367982  1.046923  ... -2.533116  1.449502   \n",
       "\n",
       "                   92        93        94        95        96        97  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.088611  0.441200 -0.260525  0.381818 -0.467972 -1.061370   \n",
       "nice         0.388893  0.837065  0.044579  1.669993 -0.244355  0.931893   \n",
       "great        0.464201  1.230113 -0.046174  1.446777  0.633704 -1.435719   \n",
       "best         0.007868  0.251197 -1.955678 -0.176787  1.074953  0.650702   \n",
       "amazing     -0.751663 -0.303269 -1.653833  0.942226  0.787624 -0.576481   \n",
       "stop         0.842667 -0.398742  0.830125  0.797854  0.109946 -0.614411   \n",
       "lies        -1.243526 -0.884184 -0.175324  0.896553 -0.575425  1.444391   \n",
       "pitiful     -0.845376  0.134653 -1.326125 -0.594806 -0.656765 -0.542460   \n",
       "nerd         1.114048  0.419245 -0.006181  0.306982  0.304988 -0.246282   \n",
       "excellent    2.085147  1.169223 -0.466419 -0.152135 -0.549252 -1.613023   \n",
       "work         1.647364 -1.082706  2.321183 -0.222166 -0.478727  2.313901   \n",
       "supreme     -0.542316  0.904640  1.039306  0.831886  0.178315  0.136362   \n",
       "quality      1.299515  0.684447  0.284661  0.531108 -0.145330  1.392461   \n",
       "bad         -1.659595 -0.767814  2.082811 -0.417833  1.285412  0.265671   \n",
       "highly       0.733722 -2.500595  0.071139 -0.161663  0.603788  1.312428   \n",
       "respectable  0.707508 -0.164226  1.165867  1.318780  0.545781  0.029950   \n",
       "\n",
       "                   98        99  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>       -1.269071  0.095704  \n",
       "nice         0.286151 -0.758150  \n",
       "great        1.304379 -0.391658  \n",
       "best        -0.161425  1.066105  \n",
       "amazing      0.725744  0.394482  \n",
       "stop         0.230818  0.812125  \n",
       "lies        -0.954079 -0.310491  \n",
       "pitiful     -0.353943  0.272664  \n",
       "nerd        -0.568133 -1.888943  \n",
       "excellent    0.022767  0.907434  \n",
       "work         0.836022 -0.296454  \n",
       "supreme     -1.032011  0.149197  \n",
       "quality     -0.272716  0.408863  \n",
       "bad         -1.730177  0.810932  \n",
       "highly       0.555324 -0.720070  \n",
       "respectable  2.065585 -0.214659  \n",
       "\n",
       "[17 rows x 100 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임베딩 가중치 확인 : 학습 전/후 Embedding 테이블과 단어별 벡터 조회\n",
    "import pandas as pd\n",
    "\n",
    "# 학습 전 임베딩 벡터\n",
    "wv = model.embedding.weight.data # Embedding 층의 가중치 행렬(단어ID x 임베딩 차원) 추출\n",
    "print(wv.shape) # (vocab_size, embedding_dim)\n",
    "\n",
    "# 특정 단어 벡터\n",
    "vocab = word_to_index.keys() # 단어 사전에서 단어만 뽑아온다.\n",
    "pd.DataFrame(wv, index=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7da044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch 학습 준비 : 텐서 변환 -> DataLoader 구성 -> 손실함수/옵티마이저 설정\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)      # 입력 시퀀ㅅ흐(정수 ID)를 LongTensor로 변환\n",
    "y = torch.tensor(labels, dtype=torch.float).unsqueeze(1)  # 라벨을 float으로 변환 후 (N,) -> (N, 1)로 차원 맞춤\n",
    "\n",
    "dataset = TensorDataset(X, y) # (X, y) 쌍을 Dataset 객체로 묶음\n",
    "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True) # 배치 단위로 섞어서 공급하는 로더\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # 출력 logit과 정답(0/1)로 이진분류 손실 계산 (시그모이드 포함)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # 모델 파라미터는 Adam으로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e27765",
   "metadata": {},
   "source": [
    "BCEWithLogitsLoss를 사요ㅕㅇ할 때에는 모델 출력이 Sigmoid를 거치지 않은 logit이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28758186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.7232493013143539\n",
      "Epoch 2: Loss 0.579514667391777\n",
      "Epoch 3: Loss 0.4901227578520775\n",
      "Epoch 4: Loss 0.35534898564219475\n",
      "Epoch 5: Loss 0.2750513684004545\n",
      "Epoch 6: Loss 0.21515660732984543\n",
      "Epoch 7: Loss 0.16199994459748268\n",
      "Epoch 8: Loss 0.10946854948997498\n",
      "Epoch 9: Loss 0.07996318209916353\n",
      "Epoch 10: Loss 0.06479165237396955\n",
      "Epoch 11: Loss 0.04956529103219509\n",
      "Epoch 12: Loss 0.03963347431272268\n",
      "Epoch 13: Loss 0.03297581849619746\n",
      "Epoch 14: Loss 0.028175192419439554\n",
      "Epoch 15: Loss 0.025760627817362547\n",
      "Epoch 16: Loss 0.020736704114824533\n",
      "Epoch 17: Loss 0.020000450778752565\n",
      "Epoch 18: Loss 0.017430904554203153\n",
      "Epoch 19: Loss 0.01656410936266184\n",
      "Epoch 20: Loss 0.014129801420494914\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0      # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:   # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()             # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)           # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch) # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                   # 역전파로 기울기 계산\n",
    "        optimizer.step()                  # 가중치(파라미터) 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()         # 배치 손실을 float으로 누적\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\") # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da47eb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                      # 평가모드\n",
    "with torch.no_grad():             # 기울기계산 비활성화\n",
    "    output = model(X)             # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)  # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()    # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "print(labels)\n",
    "\n",
    "print(pred.squeeze().detach().numpy()) # 예측 라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3d546",
   "metadata": {},
   "source": [
    "# 사전 학습된 임베딩을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61511d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, 300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_wv = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "model_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3408cdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "(17, 300)\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 매트릭스 초기화 : 사전학습 벡터로 Embedding 레이어를 채우기 위한 준비\n",
    "print(len(word_to_index)) # 어휘 크기 (vocab_size) 확인\n",
    "\n",
    "# (vocab_size, embedding_dim) 크기의 0 행렬 생성\n",
    "embedding_matrix = np.zeros((len(word_to_index), model_wv.vectors.shape[1]))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39411a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습 임베딩 매핑 : 내 단어사전을 GoogleNews 벡터로 채워 embedding_matrix 구성\n",
    "# model_wv.key_to_index['bad'] # 'bad'의 내부 인덱스 확인 (706)\n",
    "# model_wv.vectors[240]        # 특정 인덱스 벡터 직접 조회\n",
    "\n",
    "# 단어가 사전학습 모델에 있으면 임베딩 벡터(np.ndarray)를 반환, 없으면 None 반환\n",
    "def get_word_embedding(word):\n",
    "    if word in model_wv:        # 사전학습 단어가 존재하면\n",
    "        return model_wv[word]   # 해당 단어 임베딩 벡터 반환\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# get_word_embedding('bad')\n",
    "for word, index in word_to_index.items(): # 내 단어사전(단어 -> 인덱스)를 순회\n",
    "    if index >= 2:                        # 특수토큰 제외\n",
    "        emb = get_word_embedding(word)    # 사전학습 임베딩에서 해당 단어 벡터 조회\n",
    "        if emb is not None:               # 벡터가 존재하면\n",
    "            embedding_matrix[index] = emb # 내 인덱스 위치에 사전학습 벡터를 복사해서 채운다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a3eae19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;UNK&gt;</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.158203</td>\n",
       "      <td>0.105957</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.386719</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.083496</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>-0.104004</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085449</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>-0.146484</td>\n",
       "      <td>0.134766</td>\n",
       "      <td>-0.040771</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>-0.213867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.071777</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.028442</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.099609</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>-0.116699</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011475</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.289062</td>\n",
       "      <td>-0.048096</td>\n",
       "      <td>-0.199219</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.167969</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>-0.142578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best</th>\n",
       "      <td>-0.126953</td>\n",
       "      <td>0.021973</td>\n",
       "      <td>0.287109</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.127930</td>\n",
       "      <td>0.032715</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>-0.029541</td>\n",
       "      <td>0.153320</td>\n",
       "      <td>0.011292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006439</td>\n",
       "      <td>-0.033936</td>\n",
       "      <td>-0.166016</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.048584</td>\n",
       "      <td>-0.022827</td>\n",
       "      <td>-0.152344</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.088379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.135742</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>0.180664</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>0.224609</td>\n",
       "      <td>-0.229492</td>\n",
       "      <td>-0.040039</td>\n",
       "      <td>0.225586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>-0.021240</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.020142</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.006317</td>\n",
       "      <td>-0.141602</td>\n",
       "      <td>-0.150391</td>\n",
       "      <td>-0.137695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>-0.057861</td>\n",
       "      <td>0.013184</td>\n",
       "      <td>0.115234</td>\n",
       "      <td>0.069824</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.152344</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>-0.115723</td>\n",
       "      <td>0.048096</td>\n",
       "      <td>-0.004822</td>\n",
       "      <td>0.086426</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lies</th>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.012817</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>0.025513</td>\n",
       "      <td>0.017334</td>\n",
       "      <td>0.190430</td>\n",
       "      <td>0.188477</td>\n",
       "      <td>-0.143555</td>\n",
       "      <td>-0.090820</td>\n",
       "      <td>0.206055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.308594</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.202148</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.201172</td>\n",
       "      <td>0.080078</td>\n",
       "      <td>-0.105469</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.157227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitiful</th>\n",
       "      <td>0.269531</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.020996</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>-0.010925</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.139648</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.132812</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.089355</td>\n",
       "      <td>-0.065430</td>\n",
       "      <td>-0.016235</td>\n",
       "      <td>-0.107910</td>\n",
       "      <td>-0.072266</td>\n",
       "      <td>-0.094238</td>\n",
       "      <td>0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nerd</th>\n",
       "      <td>0.265625</td>\n",
       "      <td>-0.207031</td>\n",
       "      <td>-0.026611</td>\n",
       "      <td>0.419922</td>\n",
       "      <td>-0.208984</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>0.164062</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.017700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>-0.112793</td>\n",
       "      <td>-0.096680</td>\n",
       "      <td>0.255859</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>-0.030273</td>\n",
       "      <td>0.082031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>-0.212891</td>\n",
       "      <td>-0.004303</td>\n",
       "      <td>-0.180664</td>\n",
       "      <td>-0.007568</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.163086</td>\n",
       "      <td>-0.014709</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136719</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>-0.173828</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>-0.081055</td>\n",
       "      <td>0.013550</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>-0.129883</td>\n",
       "      <td>-0.215820</td>\n",
       "      <td>0.012268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>-0.075684</td>\n",
       "      <td>0.033691</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.131836</td>\n",
       "      <td>0.050537</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>-0.133789</td>\n",
       "      <td>-0.020874</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>-0.091309</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.121582</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>-0.091309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supreme</th>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.172852</td>\n",
       "      <td>0.112793</td>\n",
       "      <td>0.166016</td>\n",
       "      <td>0.058594</td>\n",
       "      <td>-0.014221</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>-0.217773</td>\n",
       "      <td>0.073730</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>-0.221680</td>\n",
       "      <td>-0.055664</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>-0.081543</td>\n",
       "      <td>0.121094</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>0.097656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>-0.259766</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.113770</td>\n",
       "      <td>0.166992</td>\n",
       "      <td>0.025024</td>\n",
       "      <td>0.067871</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145508</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.314453</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.010254</td>\n",
       "      <td>0.298828</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>-0.014099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.062988</td>\n",
       "      <td>0.124512</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.079102</td>\n",
       "      <td>0.050781</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.096191</td>\n",
       "      <td>0.220703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011353</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.076660</td>\n",
       "      <td>-0.032471</td>\n",
       "      <td>0.133789</td>\n",
       "      <td>-0.154297</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.114746</td>\n",
       "      <td>0.031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.050781</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>-0.130859</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.165039</td>\n",
       "      <td>0.118652</td>\n",
       "      <td>-0.230469</td>\n",
       "      <td>-0.225586</td>\n",
       "      <td>0.245117</td>\n",
       "      <td>-0.086914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013733</td>\n",
       "      <td>-0.013489</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.226562</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>-0.210938</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>-0.056641</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.074707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respectable</th>\n",
       "      <td>0.150391</td>\n",
       "      <td>0.285156</td>\n",
       "      <td>-0.023193</td>\n",
       "      <td>0.095703</td>\n",
       "      <td>-0.022095</td>\n",
       "      <td>0.058350</td>\n",
       "      <td>-0.155273</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.100098</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>0.056885</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.138672</td>\n",
       "      <td>-0.227539</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>-0.033447</td>\n",
       "      <td>-0.200195</td>\n",
       "      <td>-0.112305</td>\n",
       "      <td>0.103027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5    \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice         0.158203  0.105957 -0.189453  0.386719  0.083496 -0.267578   \n",
       "great        0.071777  0.208008 -0.028442  0.178711  0.132812 -0.099609   \n",
       "best        -0.126953  0.021973  0.287109  0.153320  0.127930  0.032715   \n",
       "amazing      0.073730  0.004059 -0.135742  0.022095  0.180664 -0.046631   \n",
       "stop        -0.057861  0.013184  0.115234  0.069824 -0.306641 -0.044678   \n",
       "lies         0.149414 -0.012817  0.328125  0.025513  0.017334  0.190430   \n",
       "pitiful      0.269531  0.253906 -0.020996  0.060303 -0.010925  0.217773   \n",
       "nerd         0.265625 -0.207031 -0.026611  0.419922 -0.208984  0.390625   \n",
       "excellent   -0.212891 -0.004303 -0.180664 -0.007568  0.112793  0.163086   \n",
       "work        -0.075684  0.033691 -0.064941  0.131836  0.050537  0.149414   \n",
       "supreme      0.173828  0.172852  0.112793  0.166016  0.058594 -0.014221   \n",
       "quality     -0.259766  0.271484  0.119629  0.007233  0.057373  0.113770   \n",
       "bad          0.062988  0.124512  0.113281  0.073242  0.038818  0.079102   \n",
       "highly       0.050781 -0.227539 -0.130859  0.062500 -0.165039  0.118652   \n",
       "respectable  0.150391  0.285156 -0.023193  0.095703 -0.022095  0.058350   \n",
       "\n",
       "                  6         7         8         9    ...       290       291  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
       "nice         0.083496  0.113281 -0.104004  0.178711  ... -0.085449  0.189453   \n",
       "great        0.096191 -0.116699 -0.008545  0.148438  ... -0.011475  0.064453   \n",
       "best        -0.115723 -0.029541  0.153320  0.011292  ...  0.006439 -0.033936   \n",
       "amazing      0.224609 -0.229492 -0.040039  0.225586  ...  0.018433 -0.021240   \n",
       "stop         0.048584  0.152344  0.073242 -0.100098  ...  0.100098  0.171875   \n",
       "lies         0.188477 -0.143555 -0.090820  0.206055  ... -0.308594  0.183594   \n",
       "pitiful      0.139648 -0.057617  0.312500  0.253906  ... -0.063477  0.132812   \n",
       "nerd         0.164062  0.063965  0.149414 -0.017700  ...  0.215820  0.125000   \n",
       "excellent   -0.014709 -0.078613 -0.164062  0.279297  ...  0.136719  0.000282   \n",
       "work         0.109375 -0.133789 -0.020874  0.054688  ... -0.187500  0.101562   \n",
       "supreme      0.128906 -0.217773  0.073730  0.205078  ...  0.140625 -0.221680   \n",
       "quality      0.166992  0.025024  0.067871  0.120117  ... -0.145508  0.120117   \n",
       "bad          0.050781  0.171875  0.096191  0.220703  ...  0.011353  0.341797   \n",
       "highly      -0.230469 -0.225586  0.245117 -0.086914  ... -0.013733 -0.013489   \n",
       "respectable -0.155273 -0.051025  0.100098  0.001183  ...  0.058838  0.056885   \n",
       "\n",
       "                  292       293       294       295       296       297  \\\n",
       "<PAD>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "<UNK>        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "nice        -0.146484  0.134766 -0.040771  0.032715  0.089355 -0.267578   \n",
       "great       -0.289062 -0.048096 -0.199219 -0.071289  0.064453 -0.167969   \n",
       "best        -0.166016 -0.016846 -0.048584 -0.022827 -0.152344 -0.101562   \n",
       "amazing     -0.250000 -0.020142 -0.310547 -0.207031 -0.006317 -0.141602   \n",
       "stop        -0.113281  0.064453 -0.115723  0.048096 -0.004822  0.086426   \n",
       "lies        -0.202148  0.031494 -0.164062 -0.201172  0.080078 -0.105469   \n",
       "pitiful     -0.094238  0.089355 -0.065430 -0.016235 -0.107910 -0.072266   \n",
       "nerd        -0.227539 -0.310547 -0.112793 -0.096680  0.255859  0.124023   \n",
       "excellent   -0.173828  0.004242 -0.081055  0.013550 -0.008362 -0.129883   \n",
       "work        -0.091309  0.052246 -0.164062  0.121582  0.062500  0.012024   \n",
       "supreme     -0.055664  0.034424 -0.119629 -0.081543  0.121094 -0.164062   \n",
       "quality     -0.314453  0.022095 -0.010254  0.298828  0.046387 -0.179688   \n",
       "bad         -0.090332  0.076660 -0.032471  0.133789 -0.154297 -0.063477   \n",
       "highly       0.175781  0.226562  0.086914 -0.210938 -0.111816 -0.056641   \n",
       "respectable -0.187500  0.138672 -0.227539  0.183594 -0.033447 -0.200195   \n",
       "\n",
       "                  298       299  \n",
       "<PAD>        0.000000  0.000000  \n",
       "<UNK>        0.000000  0.000000  \n",
       "nice         0.008362 -0.213867  \n",
       "great       -0.020874 -0.142578  \n",
       "best        -0.090332  0.088379  \n",
       "amazing     -0.150391 -0.137695  \n",
       "stop         0.029907  0.007812  \n",
       "lies         0.149414  0.157227  \n",
       "pitiful     -0.094238  0.028809  \n",
       "nerd        -0.030273  0.082031  \n",
       "excellent   -0.215820  0.012268  \n",
       "work         0.135742 -0.091309  \n",
       "supreme     -0.127930  0.097656  \n",
       "quality      0.000706 -0.014099  \n",
       "bad          0.114746  0.031006  \n",
       "highly       0.102539  0.074707  \n",
       "respectable -0.112305  0.103027  \n",
       "\n",
       "[17 rows x 300 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    embedding_matrix,\n",
    "    index = word_to_index.keys()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80d5658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.6907551139593124\n",
      "Epoch 2: Loss 0.5574611127376556\n",
      "Epoch 3: Loss 0.4449959769845009\n",
      "Epoch 4: Loss 0.34145500510931015\n",
      "Epoch 5: Loss 0.24103360250592232\n",
      "Epoch 6: Loss 0.1655264627188444\n",
      "Epoch 7: Loss 0.11863709054887295\n",
      "Epoch 8: Loss 0.08078970946371555\n",
      "Epoch 9: Loss 0.06137592811137438\n",
      "Epoch 10: Loss 0.04565120302140713\n",
      "Epoch 11: Loss 0.037292666267603636\n",
      "Epoch 12: Loss 0.029989425092935562\n",
      "Epoch 13: Loss 0.025312380399554968\n",
      "Epoch 14: Loss 0.021386357489973307\n",
      "Epoch 15: Loss 0.018132044468075037\n",
      "Epoch 16: Loss 0.016782503575086594\n",
      "Epoch 17: Loss 0.015784907387569547\n",
      "Epoch 18: Loss 0.01382426475174725\n",
      "Epoch 19: Loss 0.012305153300985694\n",
      "Epoch 20: Loss 0.012013128260150552\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프 : 미니배치 단위로 20 epoch 학습하며 평균 손실 출력\n",
    "for epoch in range(20):\n",
    "    epoch_loss = 0      # 손실 누적\n",
    "\n",
    "    for x_batch, y_batch in dataloader:   # 미니배치 단위로 (X, y) 가져오기\n",
    "        optimizer.zero_grad()             # 이전 배치 기울기 초기화\n",
    "        output = model(x_batch)           # 순전파로 logit 계산\n",
    "        loss = criterion(output, y_batch) # 예측 logit과 정답으로 손실 계산\n",
    "        loss.backward()                   # 역전파로 기울기 계산\n",
    "        optimizer.step()                  # 가중치(파라미터) 업데이트\n",
    "\n",
    "        epoch_loss += loss.item()         # 배치 손실을 float으로 누적\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Loss {epoch_loss / len(dataloader)}\") # epoch별 평균 손실 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6fb8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1, 0, 1]\n",
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# 평가 / 예측 : 학습된 모델로 확률 -> 0/1 예측값 생성 후 정답과 비교\n",
    "model.eval()                      # 평가모드\n",
    "with torch.no_grad():             # 기울기계산 비활성화\n",
    "    output = model(X)             # 전체 샘플에 대한 예측 logit 계산\n",
    "    prob = torch.sigmoid(output)  # logit에 0~1 확률로 변환\n",
    "    pred = (prob >= 0.5).int()    # 임계값 0.5 기준으로 이진 분류(0/1) 예측값 생성\n",
    "print(labels)\n",
    "\n",
    "print(pred.squeeze().detach().numpy()) # 예측 라벨을 1차원 numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4aba9e",
   "metadata": {},
   "source": [
    "사전학습 임베딩을 사용했을 때에도 학습 데이터 분류가 잘 되는지 파악한다.   \n",
    "만약 틀린 샘플이 있다면 해당 문장이 OOV(0벡터) 비중이 큰지 확인해봐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061b26a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
