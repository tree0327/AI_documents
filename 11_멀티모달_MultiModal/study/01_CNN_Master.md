# 01. 합성곱 신경망 (CNN) 마스터 가이드 🧠

## Part 1: 직관적 이해 (Intuition) - 요리사 비유 🍳

상상해 보세요. 당신은 **최고의 요리사**입니다. 주방에는 여러 단계의 조리 과정이 있습니다. CNN도 마치 이미지를 요리하는 주방과 같습니다.

### 1. 합성곱 층 (Convolutional Layer) = 재료 손질 및 맛보기 🧐
- **비유**: 커다란 피자 도우(이미지) 위를 작은 숟가락(필터/커널)으로 훑으며 맛을 봅니다. 숟가락이 지나간 자리에 어떤 맛(특징)이 있는지 기록합니다.
- **코드 매핑**:
  ```python
  # 숟가락(필터) 4개를 준비해서 도우(입력)를 훑습니다.
  conv_layer = nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3)
  ```
  - `kernel_size=3`: 숟가락의 크기입니다.
  - `out_channels=4`: 4명의 요리사가 각자 다른 맛(가장자리, 질감 등)을 찾습니다.

### 2. 활성화 함수 (ReLU) = 맛없는 재료 버리기 🚫
- **비유**: 맛을 본 후, **쓴맛(음수 값)**은 과감히 버리고 **맛있는 맛(양수 값)**만 남깁니다. 요리의 퀄리티를 위해 필수적인 과정입니다!
- **코드 매핑**:
  ```python
  # 쓴맛(0보다 작은 값)은 0으로 만들어 버립니다.
  x = F.relu(conv_layer(input_tensor))
  ```

### 3. 풀링 층 (Pooling Layer) = 핵심만 요약하기 📝
- **비유**: 너무 많은 정보는 머리가 아픕니다. "이 구역에서 제일 맛있는 재료 하나만 남겨!"라고 명령합니다.
- **코드 매핑**:
  ```python
  # 2x2 구역에서 가장 큰 값(가장 강한 특징) 하나만 뽑습니다.
  x = F.max_pool2d(x, kernel_size=2)
  ```

---

## Part 2: 전문가 노트 (Deep Dive) - AI 개발자를 위한 깊은 지식 💻

이제 중학생 수준을 넘어 **프로 개발자**의 시선으로 바라봅시다.

### 1. 용어 정복 (Terminology)
- **Stride (보폭)**: 필터가 이미지를 훑을 때 몇 칸씩 이동할지 결정합니다. 보폭이 크면 결과물(Feature Map)의 크기는 작아집니다.
- **Padding (패딩)**: 이미지 외곽의 정보를 잃지 않기 위해 테두리에 0(zero-padding)을 채워 넣는 기법입니다.
- **Feature Map (특징 맵)**: 합성곱 연산의 결과물입니다. 입력 이미지가 필터를 거쳐 '특징'들만 추출된 상태입니다.

### 2. CS 이론 & 하드웨어 (CS Theory)
- **공유 가중치 (Parameter Sharing)**:
  - CNN의 핵심은 **가중치 공유**입니다.
  - 일반적인 행렬 곱(Fully Connected)은 모든 픽셀마다 별도의 가중치가 필요하지만, CNN은 하나의 필터(작은 행렬)를 이미지 전체에 반복해서 사용합니다.
  - **이점**: 메모리 사용량을 획기적으로 줄이고, 학습할 파라미터 수를 감소시켜 과적합(Overfitting)을 방지합니다.

- **연산량과 GPU**:
  - 합성곱 연산은 수많은 **행렬 곱셈과 덧셈(MAC operations)**으로 이루어집니다.
  - 이는 병렬 처리에 최적화된 **GPU**가 CPU보다 훨씬 빠르게 처리할 수 있는 구조입니다. (CUDA 코어가 이 반복 연산을 동시에 수행합니다.)

### 3. 핵심 키워드 (Keywords for Search)
1. **Receptive Field (수용 영역)**: 뉴런이 원본 이미지의 어느 영역을 보고 있는지 나타내는 개념.
2. **Translation Invariance (이동 불변성)**: 고양이가 이미지의 왼쪽 위에 있든, 오른쪽 아래에 있든 똑같이 '고양이'로 인식하는 성질.
3. **Vanishing Gradient (기울기 소실)**: 층이 너무 깊어지면 학습 신호가 사라지는 현상 (ReLU가 이를 해결하는 데 도움을 줌).
