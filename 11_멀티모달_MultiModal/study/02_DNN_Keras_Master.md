# 02. 심층 신경망 (DNN) & Keras 마스터 가이드 🤖

## Part 1: 직관적 이해 (Intuition) - 자동차 조립 라인 🚗

DNN은 거대한 **자동차 조립 공장**과 같습니다. 여러 단계의 컨베이어 벨트를 거치며 고철 덩어리(데이터)가 멋진 자동차(예측 결과)로 변신합니다.

### 1. Dense Layer (완전 연결 층) = 조립 작업자들 👷
- **비유**: 수많은 작업자(뉴런)들이 일렬로 서 있습니다. 앞 단계에서 넘어온 부품(입력)을 받아 다음 단계로 넘깁니다. 모든 작업자가 서로 연결되어 정보를 주고받습니다.
- **코드 매핑**:
  ```python
  # 512명의 작업자가 784개의 부품을 받아 처리합니다.
  model.add(Dense(512, input_shape=(784,)))
  # 다음 단계에는 256명의 작업자가 대기 중입니다.
  model.add(Dense(256))
  ```

### 2. 활성화 함수 (Softmax) = 최종 검사 & 분류 📊
- **비유**: 조립이 끝난 차가 승용차인지, 트럭인지, 스포츠카인지 결정합니다. 각 차종일 **확률**을 계산해서 알려줍니다. (예: "이건 99% 확률로 스포츠카입니다!")
- **코드 매핑**:
  ```python
  # 10가지 종류(0~9 숫자) 중 하나로 분류합니다.
  model.add(Dense(10, activation='softmax'))
  ```

### 3. 손실 함수 (Loss Function) = 고객의 불만 점수 😡
- **비유**: 완성된 차를 보고 고객이 얼마나 화가 났는지 점수를 매깁니다. 점수가 높을수록(Loss가 클수록) 뭔가 잘못 만든 것입니다.
- **코드 매핑**:
  ```python
  # 분류 문제에서는 'categorical_crossentropy'라는 불만 측정기를 사용합니다.
  model.compile(loss='categorical_crossentropy')
  ```

### 4. 옵티마이저 (Optimizer) = 공장장님의 피드백 👨‍🏫
- **비유**: 불만 점수(Loss)를 보고 공장장(Adam)이 작업자들에게 지시합니다. "나사를 좀 더 조여!", "여긴 좀 느슨하게 해!" 이렇게 조립 방식(가중치)을 수정합니다.

---

## Part 2: 전문가 노트 (Deep Dive) - AI 아키텍트의 시선 🏗️

단순 조립을 넘어, 공장의 시스템을 설계하는 **아키텍트**가 되어봅시다.

### 1. 용어 정복 (Terminology)
- **One-Hot Encoding (원-핫 인코딩)**: 정답(Label)을 기계가 이해하기 쉬운 형태로 바꾸는 것. (예: 숫자 '3' -> `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`)
- **Epoch (에포크)**: 전체 데이터셋(모든 부품)을 가지고 훈련(조립 연습)을 한 번 완료하는 주기.
- **Batch Size (배치 크기)**: 한 번에 몇 개의 데이터를 묶어서 학습(피드백)할지 결정하는 크기.

### 2. CS 이론 & 최적화 (CS Theory)
- **역전파 (Backpropagation)**:
  - 딥러닝 학습의 핵심 알고리즘입니다.
  - 출력층에서 발생한 오차(Error)를 **입력층 방향으로 거꾸로 전파**하면서, 각 뉴런의 가중치(Weight)를 얼마나 수정해야 할지 미분(Chain Rule)을 통해 계산합니다.
  - **의의**: 수많은 층으로 이루어진 복잡한 네트워크도 효율적으로 학습시킬 수 있게 만든 일등 공신입니다.

- **부동소수점 연산 (Floating Point Operations)**:
  - 신경망의 가중치는 보통 `float32` (32비트 부동소수점)를 사용합니다.
  - 더 빠른 연산과 적은 메모리를 위해 `float16` (16비트)을 사용하는 **Mixed Precision** 기법도 널리 쓰입니다. (최신 NVIDIA GPU의 Tensor Core가 이를 지원합니다.)

### 3. 핵심 키워드 (Keywords for Search)
1. **Gradient Descent (경사 하강법)**: 손실 함수의 값을 최소화하기 위해 기울기(Gradient) 반대 방향으로 가중치를 이동시키는 최적화 방법.
2. **Overfitting (과적합)**: 모델이 학습 데이터에만 너무 익숙해져서, 새로운 데이터(시험 문제)는 잘 못 맞추는 현상. (해결책: Dropout, 조기 종료 등)
3. **Universal Approximation Theorem (보편 근사 정리)**: 충분히 많은 뉴런과 은닉층이 있다면, 신경망은 어떤 복잡한 함수도 근사할 수 있다는 이론적 증명.
