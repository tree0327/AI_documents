{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f495c95a",
   "metadata": {},
   "source": [
    "# 딥러닝 최적 모델 학습 (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7eb7d",
   "metadata": {},
   "source": [
    "손실함수의 목표 : 손실이 0이 되는 것 (=경사하강법의 목표)\n",
    "일반 경사하강법 : 전체 데이터 대상 / 배치 전체를 기준으로 넣음 => 기울기 계산 -> 학습 속도 느림\n",
    "\n",
    "확률적 경사하강법(SGD) :  랜덤성 부여. 매 반복마다 무작위로 한개(또는 작은 미니배치)의 샘플 데이터셋 추출해 가중치 계산 -> 학습 속도 빠름\n",
    "\n",
    "일반 경사하강법 VS 확률적 경사하강법(SGD)\n",
    "일반 경사하강법은 SGD 보다 안정성이 높다.\n",
    "\n",
    "$ v \\leftarrow \\alpha v - \\eta \\frac{\\partial L}{\\partial w} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62679da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [ 0.498 -0.296]\n"
     ]
    }
   ],
   "source": [
    "# SGD 최적화 함수 업데이트\n",
    "import numpy as np\n",
    "\n",
    "# 새 가중치를 반환하는 함수(현재 가중치, 기울기, lr:선택)\n",
    "def sgd_update(w, grad, lr=0.01):\n",
    "    return w - lr * grad\n",
    "\n",
    "# 가중치와 기울기\n",
    "w = np.array([0.5, -0.3])\n",
    "grad = np.array([0.2, -0.4])\n",
    "\n",
    "# SGD 업데이트 적용\n",
    "w_new = sgd_update(w, grad)\n",
    "print(\"Updated weights:\", w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44ded0",
   "metadata": {},
   "source": [
    "## 모멘텀(Momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b48c4eb",
   "metadata": {},
   "source": [
    "## Momentum 경사하강법 요약\n",
    "\n",
    "$$\n",
    "\\mathbf{v} \\leftarrow \\alpha \\mathbf{v} - \\eta \\frac{\\partial L}{\\partial \\mathbf{w}}\n",
    "$$\n",
    "\n",
    "- **$\\mathbf{v}$** : 이전 이동 방향을 누적한 속도(관성)\n",
    "- **$\\alpha$** : 모멘텀 계수 (보통 0.9)\n",
    "- **$\\eta$** : 학습률\n",
    "- **$\\frac{\\partial L}{\\partial \\mathbf{w}}$** : 손실 함수의 기울기\n",
    "\n",
    "👉 과거 이동 방향(관성)과 현재 기울기를 결합해  \n",
    "더 빠르고 안정적으로 최적점으로 이동\n",
    "\n",
    "---\n",
    "\n",
    "### 가중치 업데이트\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\mathbf{v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2982b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [ 0.498 -0.296]\n",
      "Updated velocity: [-0.002  0.004]\n"
     ]
    }
   ],
   "source": [
    "# momentum 업데이트 함수: 누적속도 V를 이용해 기울기 변동을 완화하고 더 안정적으로 가중치를 갱신\n",
    "def momentum_update(w, v, grad, lr=0.01, momentum=0.9):\n",
    "    v = momentum * v - lr * grad        # 이전 이동방향 유지 + 현재 기울기 반영\n",
    "    w += v                              # 속도 만큼 가중치 업데이트\n",
    "    return w, v\n",
    "\n",
    "# 초기값 설정\n",
    "w = np.array([0.5, -0.3])\n",
    "v = np.array([0.0, 0.0])\n",
    "grad = np.array([0.2, -0.4])\n",
    "\n",
    "# 모멘텀 업데이트 적용\n",
    "w_new, v_new = momentum_update(w, v, grad)\n",
    "print(\"Updated weights:\", w_new)\n",
    "print(\"Updated velocity:\", v_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2506daa3",
   "metadata": {},
   "source": [
    "### 3. AdaGrad\n",
    "- 희소한 특성(0)이 많은 경우 : 원핫 인코딩/임베딩 사용, 추천시스템 user/item ID => 파라미터가 학습이 잘되는 경향 있음.\n",
    "- 학습률을 개별 파라미터마다 다르게 적용하는 기법 (자주 업뎃 특성 -> lr ⬇️, 희소한 특성 -> lr ⬆️)\n",
    "\n",
    "- 기울기의 제곱합이 계속 누적되면서 학습률 급격히 감소해 학습 멈춤 가능성  (AdaGrad 단점 보완 모델 : RMSprop)\n",
    "    - (RMSprop(0이되어 멈추는 문제 해결) -> Adam(=요즘 많이 쓰이는 기법))   \n",
    "                            - Adam : RMSprop(보폭조절) + Momentum\n",
    "\n",
    "\n",
    " #### $$ w \\leftarrow w - \\frac{\\eta}{\\sqrt{h} + \\epsilon} \\frac{\\partial L}{\\partial w} $$\n",
    " - $\\epsilon$ : 0으로 나누는 것 방지용 작은 수(수치 안정화)\n",
    " - 학습률 조절로 분모가 작아지면 학습률 ⬆️ \n",
    " - 결론 : 기울기 크기에 따라 학습률 나눠줌으로 자주 업뎃되면 속도를 낮춰준다. 자주 업뎃 안되면 속도 높여주는 방법\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "- **$h$** : 누적 제곱 기울기\n",
    "- **$w$** : 업데이트할 파라미터(가중치)\n",
    "- **$\\eta$** : 기본 학습률(전역 learning rate)\n",
    "- **$\\frac{\\partial L}{\\partial w}$** : 현재 기울기(gradient)\n",
    "- **$\\epsilon$** : 0으로 나누는 것을 방지하기 위한 작은 상수(수치 안정화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a8ab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [ 0.49 -0.29]\n",
      "Updated 누적 제곱 기울기: [0.04 0.16]\n"
     ]
    }
   ],
   "source": [
    "# AdaGrad 업데이트 함수 : 누적 제곱 기울기(h)를 이용해 파라미터 별 학습 률을 자동으로 조절한다.\n",
    "def adagrad_update(w, grad, h, lr=0.01, epsilon=1e-8):\n",
    "    h += grad ** 2\n",
    "    w -= (lr / (np.sqrt(h) + epsilon)) * grad\n",
    "    return w, h\n",
    "\n",
    "# 초기값 설정\n",
    "w = np.array([0.5, -0.3])\n",
    "h = np.array([0.0, 0.0])\n",
    "grad = np.array([0.2, -0.4])\n",
    "\n",
    "# AdaGrad 업데이트 적용\n",
    "w_new, h_new = adagrad_update(w, grad, h)\n",
    "print(\"Updated weights:\", w_new)\n",
    "print(\"Updated 누적 제곱 기울기:\", h_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6d9aa",
   "metadata": {},
   "source": [
    "## RMSprop 기울기 제곱 평균(EMA) 갱신 요약\n",
    "\n",
    "$$\n",
    "h_t \\leftarrow \\beta h_{t-1} + (1 - \\beta)\\left(\\frac{\\partial L}{\\partial w}\\right)^2\n",
    "$$\n",
    "\n",
    "- **$h_t$** : $t$번째 스텝에서의 기울기 제곱 평균 (지수이동평균, EMA)\n",
    "- **$h_{t-1}$** : 이전 스텝까지 누적된 기울기 제곱 평균\n",
    "- **$\\left(\\frac{\\partial L}{\\partial w}\\right)^2$** : 현재 스텝의 기울기 제곱 (기울기 크기 정보)\n",
    "- **$\\beta$** : 감쇠율(decay rate, 일반적으로 0.9)\n",
    "  - $\\beta$가 클수록 과거 기울기 정보를 더 많이 유지\n",
    "  - $(1-\\beta)$는 현재 기울기 제곱을 반영하는 비율\n",
    "\n",
    "### 부가 설명\n",
    "RMSprop은 기울기 제곱의 단순 누적 대신 지수이동평균을 사용하여  \n",
    "과거의 큰 기울기 영향은 점차 줄이고, 최근 기울기 크기를 중심으로  \n",
    "학습률을 조절한다.  \n",
    "이를 통해 학습률이 지나치게 빠르게 감소하는 문제를 완화하고,  \n",
    "비정상적인 진동이나 발산을 줄이는 데 목적이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a9e18",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ec1fd",
   "metadata": {},
   "source": [
    "## RMSprop 가중치 업데이트 요약\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\frac{\\eta}{\\sqrt{h_t} + \\epsilon} \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "- **$w$** : 업데이트할 파라미터(가중치)\n",
    "- **$\\eta$** : 기본 학습률(전역 learning rate)\n",
    "- **$\\frac{\\partial L}{\\partial w}$** : 현재 기울기(gradient)\n",
    "- **$h_t$** : 최근 기울기 제곱의 지수이동평균(EMA)\n",
    "- **$\\epsilon$** : 0으로 나누는 것을 방지하기 위한 작은 상수(수치 안정화)\n",
    "\n",
    "### 부가 설명\n",
    "RMSprop은 현재 기울기의 방향은 그대로 사용하면서,  \n",
    "최근 기울기 크기의 평균인 $h_t$로 나누어 파라미터별 스텝 크기를 자동으로 조절한다.  \n",
    "기울기가 자주 크게 변하는 방향에서는 학습률이 줄어들고,  \n",
    "변화가 작은 방향에서는 상대적으로 큰 스텝을 유지하여  \n",
    "학습의 안정성과 수렴 속도를 개선한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91472412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop 업데이트 합수 : 지수이동평균(EMA) 으로 제곱기울기(h)를 누적해 AdaGrad의 학습률 급감 문제를 완화하여 W값을 갱신\n",
    "def rmsprop_update(w, h, grad, lr=0.01, beta=0.9, epsilon=1e-8):\n",
    "    h = beta * h + (1 - beta) * (grad ** 2)         # 지수이동평균 EMA\n",
    "    w -= (lr / (np.sqrt(h) + epsilon)) * grad       # 스텝 크기 조정\n",
    "    return w, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c1ddc",
   "metadata": {},
   "source": [
    "## Adam 옵티마이저 업데이트 과정 요약\n",
    "\n",
    "- **개요**  \n",
    "  Adam 옵티마이저가 손실 함수  \n",
    "  $$\n",
    "  L(w) = w^2\n",
    "  $$  \n",
    "  에서 가중치 $w$를 반복적으로 업데이트하며 **최솟값 $w=0$** 방향으로 수렴하는 과정을 나타냄.\n",
    "\n",
    "- **구성 요소**\n",
    "  - **검은 곡선**: 손실 함수 $L(w)=w^2$ (포물선)\n",
    "  - **x축**: 가중치 $w$\n",
    "  - **y축**: 손실 $L(w)$\n",
    "  - **빨간 점**: 각 학습 step에서 갱신된 가중치 $w$의 위치(업데이트 궤적)\n",
    "\n",
    "- **부가 설명**\n",
    "  Adam은 기울기의 **1차 모멘트(평균)**와 **2차 모멘트(제곱 평균)**를 동시에 고려해\n",
    "  스텝 크기를 자동 조절한다.  \n",
    "  이로 인해 초기에는 비교적 큰 스텝으로 빠르게 이동하고,\n",
    "  최솟값 근처에서는 스텝이 줄어들며 안정적으로 $w=0$에 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20540a74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd84e8b",
   "metadata": {},
   "source": [
    "## Adam 옵티마이저 – 1차 모멘트(기울기 EMA) 업데이트\n",
    "\n",
    "Adam(Adaptive Moment Estimation)은 **Momentum과 RMSprop을 결합한 방법**으로,  \n",
    "기울기의 **1차 모멘트(이동평균)**와 **2차 모멘트(제곱 이동평균)**를 함께 사용해\n",
    "안정적이고 빠른 학습을 수행한다.\n",
    "\n",
    "### 1차 모멘트 업데이트 식\n",
    "$$\n",
    "m \\leftarrow \\beta_1 m + (1 - \\beta_1)\\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "### 변수 설명\n",
    "- **$m$** : 기울기의 지수이동평균(EMA), 모멘텀 역할  \n",
    "  → 과거 기울기 방향을 누적해 현재 업데이트 방향을 안정화\n",
    "- **$\\frac{\\partial L}{\\partial w}$** : 현재 스텝의 기울기(gradient)\n",
    "- **$\\beta_1$** : 1차 모멘트의 감쇠율(decay rate, 일반적으로 0.9)\n",
    "  - $\\beta_1$가 클수록 과거 기울기 정보를 더 오래 유지\n",
    "  - $(1-\\beta_1)$는 현재 기울기를 반영하는 비율\n",
    "\n",
    "### 부가 설명\n",
    "1차 모멘트는 **기울기의 방향 정보**를 부드럽게 평균내어,\n",
    "노이즈가 큰 경우에도 업데이트 방향이 급격히 변하지 않도록 한다.  \n",
    "이로 인해 단순 경사하강법보다 진동이 줄고,  \n",
    "Momentum과 유사한 효과로 수렴 속도와 안정성이 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bbaae",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b53212",
   "metadata": {},
   "source": [
    "## Adam 옵티마이저 – 2차 모멘트(기울기 제곱 EMA) 업데이트\n",
    "\n",
    "### 2차 모멘트 업데이트 식\n",
    "$$\n",
    "v \\leftarrow \\beta_2 v + (1 - \\beta_2)\\left(\\frac{\\partial L}{\\partial w}\\right)^2\n",
    "$$\n",
    "\n",
    "### 변수 설명\n",
    "- **$v$** : 기울기 제곱의 지수이동평균(EMA)  \n",
    "  → 최근 기울기의 **크기(scale)**를 추적하는 통계값\n",
    "- **$\\left(\\frac{\\partial L}{\\partial w}\\right)^2$** : 현재 기울기를 제곱한 값  \n",
    "  → 부호(±)를 제거하고 크기 정보만 반영\n",
    "- **$\\beta_2$** : 2차 모멘트의 감쇠율(decay rate, 일반적으로 0.999)\n",
    "  - $\\beta_2$가 클수록 과거 정보의 비중이 커짐\n",
    "  - 평균이 더 부드럽게 변하며 급격한 스케일 변화가 완화됨\n",
    "\n",
    "### 부가 설명\n",
    "2차 모멘트는 학습률의 **분모 역할**을 하여,\n",
    "기울기가 자주 크게 변하는 파라미터에서는 스텝 크기를 줄이고  \n",
    "변화가 작은 파라미터에서는 상대적으로 큰 스텝을 유지하게 한다.  \n",
    "이를 통해 Adam은 학습의 안정성과 수렴 특성을 동시에 확보한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c60e29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6dd35",
   "metadata": {},
   "source": [
    "## Adam 옵티마이저 – 가중치 업데이트\n",
    "\n",
    "### 가중치 업데이트 식\n",
    "$$\n",
    "w \\leftarrow w - \\frac{\\eta}{\\sqrt{v} + \\epsilon} \\, m\n",
    "$$\n",
    "\n",
    "### 변수 설명\n",
    "- **$w$** : 업데이트할 가중치(파라미터)\n",
    "- **$\\eta$** : 기본 학습률(전역 learning rate)\n",
    "- **$m$** : 1차 모멘트  \n",
    "  → 기울기의 지수이동평균(EMA), 업데이트 **방향**과 관성을 결정\n",
    "- **$v$** : 2차 모멘트  \n",
    "  → 기울기 제곱의 지수이동평균(EMA), 업데이트 **스케일(크기)**을 조절\n",
    "- **$\\epsilon$** : 0으로 나누는 문제를 방지하기 위한 작은 상수(수치 안정화)\n",
    "\n",
    "### 부가 설명\n",
    "Adam은 1차 모멘트 $m$으로 이동 방향을 결정하고,  \n",
    "2차 모멘트 $v$로 스텝 크기를 조절하여 파라미터를 갱신한다.  \n",
    "이 구조를 통해 노이즈가 큰 상황에서도 안정적인 업데이트가 가능하며,  \n",
    "Momentum과 RMSprop의 장점을 동시에 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d944c9b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
