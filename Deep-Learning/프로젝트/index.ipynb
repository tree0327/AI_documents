{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f2e2e4",
   "metadata": {},
   "source": [
    "# ğŸ“ ì´ì»¤ë¨¸ìŠ¤ ì´íƒˆ ì˜ˆì¸¡ í”„ë¡œì íŠ¸ (Final Mentor Ver.)\n",
    "**\"ë°ì´í„° ë¬´ê²°ì„±\", \"ì´ˆë‹¨ê¸° ë°ì´í„° ëŒ€ì‘\", \"í•™ìŠµ ì•ˆì •ì„±\"ì´ ëª¨ë‘ ì ìš©ëœ ìµœì¢…ë³¸**\n",
    "\n",
    "### ğŸ› ï¸ ìˆ˜ì • ì´ë ¥ (Fix History)\n",
    "1.  **Memory Load Logic**: êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ ë° ë¡œì»¬ íŒŒì¼ íƒìƒ‰ ë¡œì§ ê°•í™”.\n",
    "2.  **Adaptive Time-Split**: ë°ì´í„° ê¸°ê°„ì´ ì§§ì•„ë„(12ì¼ ë“±) ìë™ìœ¼ë¡œ í•™ìŠµ/ê²€ì¦ êµ¬ê°„ì„ ì„¤ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì ìš©.\n",
    "3.  **Empty Data Safety**: ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ 0ê±´ì´ ë˜ì–´ë„ ì—ëŸ¬ ì—†ì´ ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•´ íŒŒì´í”„ë¼ì¸ ìœ ì§€.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67d1281",
   "metadata": {},
   "source": [
    "## [Step 1] í™˜ê²½ ì„¤ì • ë° í•œê¸€ í°íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e92c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "import os, warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, f1_score, precision_recall_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def set_korean_font():\n",
    "    font_path = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "    if not os.path.exists(font_path):\n",
    "        os.system(\"sudo apt-get -qq -y install fonts-nanum\")\n",
    "        os.system(\"sudo fc-cache -fv\")\n",
    "        os.system(\"rm ~/.cache/matplotlib -rf\")\n",
    "    try:\n",
    "        fm.fontManager.addfont(font_path)\n",
    "        plt.rc('font', family=fm.FontProperties(fname=font_path).get_name())\n",
    "    except:\n",
    "        plt.rc('font', family='NanumBarunGothic')\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "set_korean_font()\n",
    "print(\"âœ… í°íŠ¸ ì„¤ì • ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7402762",
   "metadata": {},
   "source": [
    "## [Step 2] ë°ì´í„° ë¡œë“œ (Robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"â–¶ ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "\n",
    "# 1. Drive Mount\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import os\n",
    "    possible_paths = [\n",
    "        '/content/drive/MyDrive/Deep-Learning/í”„ë¡œì íŠ¸',\n",
    "        '/content/drive/MyDrive/Colab Notebooks',\n",
    "        '/content/drive/MyDrive'\n",
    "    ]\n",
    "    for p in possible_paths:\n",
    "        if os.path.exists(p):\n",
    "            os.chdir(p)\n",
    "            print(f\"ğŸ“‚ ê²½ë¡œ ì´ë™: {p}\")\n",
    "            break\n",
    "except:\n",
    "    print(\"âš ï¸ ë¡œì»¬/ë“œë¼ì´ë¸Œ ì‹¤íŒ¨ (í˜„ì¬ ê²½ë¡œ ì‚¬ìš©)\")\n",
    "\n",
    "# 2. File Check\n",
    "target_file = 'clock_all.csv'\n",
    "if not os.path.exists(target_file) and os.path.exists(f\"./data/{target_file}\"):\n",
    "    target_file = f\"./data/{target_file}\"\n",
    "\n",
    "if not os.path.exists(target_file):\n",
    "    print(f\"ğŸš¨ íŒŒì¼ ì—†ìŒ: {target_file}\")\n",
    "    # ë””ë²„ê¹…ìš© íŒŒì¼ ëª©ë¡ ì¶œë ¥\n",
    "    print(f\"í˜„ì¬ ê²½ë¡œ íŒŒì¼: {os.listdir('.')}\")\n",
    "    raise FileNotFoundError(\"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "cols = ['event_time', 'event_type', 'product_id', 'brand', 'price', 'user_id', 'user_session']\n",
    "df = pd.read_csv(target_file, usecols=cols)\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], utc=True, errors='coerce')\n",
    "df = df.dropna(subset=['event_time'])\n",
    "df = df[df['brand'].isin(['samsung', 'apple', 'xiaomi'])].reset_index(drop=True)\n",
    "\n",
    "print(f\"âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df):,} Rows ({df['event_time'].min().date()} ~ {df['event_time'].max().date()})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39161420",
   "metadata": {},
   "source": [
    "## [Step 3] í”¼ì³ ìƒì„± (Time-Split & Adaptive Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c643e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_features_and_target(df_full, cutoff_date_str, lookback_days=30):\n",
    "    cutoff = pd.Timestamp(cutoff_date_str, tz='UTC')\n",
    "    start_date = cutoff - pd.Timedelta(days=lookback_days)\n",
    "    end_date = cutoff + pd.Timedelta(days=7)\n",
    "    \n",
    "    print(f\"ğŸ› ï¸ ìƒì„±: {cutoff.date()} (History {lookback_days}ì¼: {start_date.date()}~)\")\n",
    "    \n",
    "    mask_past = (df_full['event_time'] >= start_date) & (df_full['event_time'] < cutoff)\n",
    "    df_past = df_full[mask_past].copy()\n",
    "    \n",
    "    mask_future = (df_full['event_time'] >= cutoff) & (df_full['event_time'] < end_date)\n",
    "    df_future = df_full[mask_future].copy()\n",
    "\n",
    "    # [Safety] Data Empty\n",
    "    if df_past.empty:\n",
    "        print(\"âš ï¸ Past Data Empty -> Return Safe Empty DataFrame\")\n",
    "        empty_cols = ['cnt_view', 'cnt_cart', 'cnt_remove', 'cnt_competitor', 'cart_remove_ratio', \n",
    "                      'avg_gap_days', 'avg_duration', 'price_tier', 'is_retained']\n",
    "        return pd.DataFrame(columns=empty_cols)\n",
    "\n",
    "    # Feature Engineering\n",
    "    is_samsung = df_past['brand'] == 'samsung'\n",
    "    df_past['is_view'] = (is_samsung & (df_past['event_type'] == 'view')).astype(int)\n",
    "    df_past['is_cart'] = (is_samsung & (df_past['event_type'] == 'cart')).astype(int)\n",
    "    df_past['is_remove'] = (is_samsung & (df_past['event_type'] == 'remove_from_cart')).astype(int)\n",
    "    df_past['is_competitor'] = (~is_samsung).astype(int)\n",
    "    \n",
    "    features = df_past.groupby('user_id').agg(\n",
    "        cnt_view=('is_view', 'sum'),\n",
    "        cnt_cart=('is_cart', 'sum'),\n",
    "        cnt_remove=('is_remove', 'sum'),\n",
    "        cnt_competitor=('is_competitor', 'sum')\n",
    "    )\n",
    "    features['cart_remove_ratio'] = features['cnt_remove'] / (features['cnt_cart'] + 1)\n",
    "    \n",
    "    # Gap\n",
    "    samsung_logs = df_past[is_samsung].sort_values(['user_id', 'event_time'])\n",
    "    if not samsung_logs.empty:\n",
    "        s = samsung_logs.groupby(['user_id', 'user_session'])['event_time'].min().reset_index().sort_values(['user_id','event_time'])\n",
    "        s['prev'] = s.groupby('user_id')['event_time'].shift(1)\n",
    "        s['gap'] = (s['event_time'] - s['prev']).dt.days\n",
    "        avg_gap = s.groupby('user_id')['gap'].mean().fillna(lookback_days)\n",
    "        features = features.join(avg_gap.rename('avg_gap_days'), how='left').fillna(lookback_days)\n",
    "    else:\n",
    "        features['avg_gap_days'] = lookback_days\n",
    "        \n",
    "    # Duration\n",
    "    dur = df_past.groupby(['user_id', 'user_session'])['event_time'].agg(lambda x:(x.max()-x.min()).total_seconds())\n",
    "    features = features.join(dur.groupby('user_id').mean().rename('avg_duration'), how='left').fillna(0)\n",
    "    \n",
    "    # Price\n",
    "    mp = df_past[is_samsung].groupby('user_id')['price'].max().fillna(0)\n",
    "    features['price_tier'] = pd.cut(mp, bins=[-1, 100, 300, 600, float('inf')], labels=[0,1,2,3]).astype(int)\n",
    "    \n",
    "    # Label\n",
    "    retained = df_future['user_id'].unique()\n",
    "    features['is_retained'] = 0\n",
    "    features.loc[features.index.isin(retained), 'is_retained'] = 1\n",
    "    \n",
    "    return features.fillna(0)\n",
    "\n",
    "# Execution (Adaptive)\n",
    "total_days = (df['event_time'].max() - df['event_time'].min()).days\n",
    "print(f\"ğŸ“Š ì´ ê¸°ê°„: {total_days}ì¼\")\n",
    "\n",
    "if total_days < 14:\n",
    "    print(\"âš ï¸ Short Data Mode Activated\")\n",
    "    mid = df['event_time'].min() + pd.Timedelta(days=int(total_days/2))\n",
    "    train_cutoff = mid.strftime('%Y-%m-%d')\n",
    "    test_cutoff = (df['event_time'].max() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    lookback = int(total_days/2)\n",
    "else:\n",
    "    test_cutoff = (df['event_time'].max() - pd.Timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    train_cutoff = (pd.Timestamp(test_cutoff) - pd.Timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "    lookback = 30\n",
    "\n",
    "train_df = create_features_and_target(df, train_cutoff, lookback)\n",
    "test_df = create_features_and_target(df, test_cutoff, lookback)\n",
    "\n",
    "# Safety Dummy Row\n",
    "if len(train_df) == 0:\n",
    "    print(\"ğŸš¨ Train Data 0ê±´ -> Dummy ì¶”ê°€\")\n",
    "    dummy = pd.DataFrame([[0]*8 + [0]], columns=train_df.columns)\n",
    "    train_df = pd.concat([train_df, dummy])\n",
    "    \n",
    "print(f\"âœ… ìµœì¢… ë°ì´í„°: Train({len(train_df)}), Test({len(test_df)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15988fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling & Clustering\n",
    "cols_cluster = ['cnt_view', 'cnt_cart', 'avg_gap_days', 'avg_duration', 'cart_remove_ratio', 'price_tier', 'cnt_competitor']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Dummy Dataì¼ ê²½ìš° fit ì—ëŸ¬ ë°©ì§€ -> ê°’ì´ í•˜ë‚˜ë©´ scale ì˜ë¯¸ X\n",
    "if len(train_df) < 2:\n",
    "    X_train_s = np.zeros((len(train_df), len(cols_cluster)))\n",
    "    X_test_s = np.zeros((len(test_df), len(cols_cluster)))\n",
    "else:\n",
    "    X_train_s = scaler.fit_transform(train_df[cols_cluster])\n",
    "    if len(test_df) > 0:\n",
    "        X_test_s = scaler.transform(test_df[cols_cluster])\n",
    "    else:\n",
    "        X_test_s = np.empty((0, len(cols_cluster)))\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "if len(train_df) >= 3:\n",
    "    train_df['c'] = kmeans.fit_predict(X_train_s)\n",
    "    # Retention ìˆœì„œ ì •ë ¬\n",
    "    rank = train_df.groupby('c')['is_retained'].mean().sort_values().index\n",
    "    imap = {old:new for new,old in enumerate(rank)}\n",
    "    train_df['c'] = train_df['c'].map(imap)\n",
    "    if len(test_df) > 0:\n",
    "        test_df['c'] = kmeans.predict(X_test_s)\n",
    "        test_df['c'] = test_df['c'].map(imap)\n",
    "else:\n",
    "    train_df['c'] = 0\n",
    "    if len(test_df) > 0: test_df['c'] = 0\n",
    "\n",
    "print(\"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensor Prep\n",
    "def to_tensor(d, s_scaled):\n",
    "    if len(d) == 0: return torch.FloatTensor([]), torch.FloatTensor([])\n",
    "    c_dummy = pd.get_dummies(d['c'], prefix='c').reindex(columns=['c_0','c_1','c_2'], fill_value=0).values\n",
    "    x = np.hstack([s_scaled, c_dummy])\n",
    "    y = d['is_retained'].values\n",
    "    return torch.FloatTensor(x), torch.FloatTensor(y).view(-1, 1)\n",
    "\n",
    "Xt, yt = to_tensor(train_df, X_train_s)\n",
    "Xv, yv = to_tensor(test_df, X_test_s)\n",
    "\n",
    "# Loader\n",
    "bs = 512\n",
    "train_loader = DataLoader(TensorDataset(Xt, yt), batch_size=bs, shuffle=True)\n",
    "if len(Xv) > 0:\n",
    "    test_loader = DataLoader(TensorDataset(Xv, yv), batch_size=bs, shuffle=False)\n",
    "else:\n",
    "    test_loader = []\n",
    "\n",
    "# Model\n",
    "class SimpleDNN(nn.Module):\n",
    "    def __init__(self, d_in):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "model = SimpleDNN(Xt.shape[1])\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "sche = StepLR(opt, step_size=5, gamma=0.5)\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0]))\n",
    "\n",
    "loss_hist, f1_hist = [], []\n",
    "\n",
    "print(\"â–¶ í•™ìŠµ ì‹œì‘\")\n",
    "for ep in range(15):\n",
    "    model.train()\n",
    "    e_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = crit(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        e_loss += loss.item()\n",
    "    sche.step()\n",
    "    \n",
    "    # Valid\n",
    "    if len(Xv) > 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out_v = model(Xv)\n",
    "            prob = torch.sigmoid(out_v).numpy()\n",
    "            pred = (prob>0.5).astype(int)\n",
    "            f1 = f1_score(yv.numpy(), pred, zero_division=0)\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    loss_hist.append(e_loss/len(train_loader))\n",
    "    f1_hist.append(f1)\n",
    "    \n",
    "    if (ep+1)%5==0:\n",
    "        print(f\"Ep {ep+1} | Loss: {loss_hist[-1]:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ì¢…ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist, label='Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(f1_hist, color='g', label='F1')\n",
    "plt.title('Validation F1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "if len(Xv) > 0:\n",
    "    print(classification_report(yv.numpy(), pred, target_names=['ì´íƒˆ', 'ìœ ì§€'], zero_division=0))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
