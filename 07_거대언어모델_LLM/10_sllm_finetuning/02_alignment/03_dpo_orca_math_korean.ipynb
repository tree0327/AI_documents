{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fc943b",
   "metadata": {},
   "source": [
    "# DPO orca-math-korean\n",
    "\n",
    "**Dataset:**\n",
    "- https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k\n",
    "- https://huggingface.co/datasets/kuotient/orca-math-korean-dpo-pairs\n",
    "\n",
    "**Model:**\n",
    "- https://huggingface.co/soka0000/vclm-korean-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b70152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers hf_transfer accelerate peft trl wandb scikit-learn - Uq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c046ea",
   "metadata": {},
   "source": [
    "## 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ef2b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'system': '당신은 인공지능 어시스턴트입니다.', 'question': '한 배럴에는 12리터(L)와 400밀리리터(ml)의 석유, B 배럴에는 7600밀리리터(ml)의 석유가 들어 있습니다. A 배럴과 B 배럴의 석유 양을 같게 하려면 A 배럴에서 B 배럴로 몇 리터(L)를 옮겨야 합니까?', 'chosen': '먼저 모든 측정값을 동일한 단위로 변환하여 계산을 쉽게 해봅시다. 모든 것을 밀리리터(ml)로 변환하겠습니다.\\n\\n배럴에는 12리터와 400밀리리터가 들어 있습니다. 1리터는 1000밀리리터와 같으므로 12리터를 밀리리터로 변환할 수 있습니다:\\n12리터 = 12 * 1000밀리리터 = 12000밀리리터\\n\\n이제 이미 밀리리터 단위로 표시된 400밀리리터를 더합니다:\\n12000밀리리터 + 400밀리리터 = 12400밀리리터\\n\\n따라서 배럴에는 총 12400밀리리터의 석유 가 들어 있습니다.\\n\\nB 배럴에는 7600밀리리터의 석유가 들어 있습니다.\\n\\n두 배럴의 석유 양을 동일하게 하려면 두 양의 평균을 구해야 합니다:\\n두 배럴의 총량 = 12400밀리리터(A배럴) + 7600밀리리터(B배럴)\\n두 배럴의 총량 = 20000밀리리터\\n\\n이제 이 총량을 2로 나누어 각 배럴에 해당하는 양을 구합니다:\\n각 배럴의 동일한 양 = 20000밀리리터 / 2\\n각 배럴의 동일한 양 = 10000밀리리터\\n\\n현재 A 배럴에는 12400밀리리터가 있으므로, 두 배럴의 양이 각각 10000밀리리터가 되도록 일부를 B 배럴로 이동해야 합니다.\\n\\nA에서 B로 이동할 양 = 12400밀리리터(A 배럴) - 10000밀리리터(동일한 양)\\nA에서 B로 이동할 양 = 2400밀리리터\\n\\n따라서 두 배럴의 석유 양을 동일하게 하기 위해 A 배럴에서 B 배럴로 2400밀리리터(또는 2.4리터)를 이동해야 합니다.', 'rejected': ' 먼저 B 배럴의 석유 양을 리터로 변환하여 A 배럴의 석유 양과 비교해야 합니다.\\n\\nB 배럴의 석유 양은 7600밀리리터(ml)입니다. 1리터에는 1000밀리리터가 있으므로 7600밀리리터를 리터로 변환할 수 있습니다:\\n\\n7600ml = 7600 / 1000 L\\n7600ml = 7.6L\\n\\n이제 A 배럴에는 12리터와 400밀리리터의 석유가 들어 있다는 것을 알 수 있습니다. 400밀리리터를 리터로 변환해야 합니다:\\n\\n400ml = 400 / 1000 L\\n400ml = 0.4L\\n\\n따라서 A 배럴의 석유 총량은 다음과 같습니다:\\n\\n12L + 0.4L = 12.4L\\n\\nA 배럴과 B 배럴의 석유 양을 같게 하려면 A 배럴에서 B 배럴로 석유를 옮겨야 합니다. A 배럴의 석유 양은 12.4L이고 B 배럴의 석유 양은 7.6L이므로, 두 양이 같으려면 A 배럴에서 B 배럴로 12.4L - 7.6L을 옮겨야 합니다:\\n\\n12.4L - 7.6L = 4.8L\\n\\n따라서 A 배럴에서 B 배럴로 4.8리터의 석유를 옮겨야 두 배럴의 석유 양이 같아집니다.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset  # HuggingFace 데이터 셋 로드 함수\n",
    "\n",
    "# DPO용 데이터 (chosen/rejected) 학습 데이터 로드\n",
    "dataset = load_dataset(\"kuotient/orca-math-korean-dpo-pairs\", split=\"train\")\n",
    "SAMPLE_SIZE = 10000\n",
    "dataset = dataset.select(range(SAMPLE_SIZE))  # 앞에서부터 10000개만 선택\n",
    "print(len(dataset))\n",
    "print(dataset[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd813aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': '당신은 인공지능 어시스턴트입니다.', 'question': '제임스는 일주일 동안 500개의 깡통을 모아 깡통 집을 짓기로 결심합니다. 첫날에는 50개의 깡통을 모았습니다. 둘째 날에는 첫날에 모은 깡통의 특정 배수를 모았습니다. 셋째 날에는 둘째 날에 모은 깡통보다 50개를 더 모았습니다. 그는 일주일의 나머지 날에도 매일 50개씩 같은 수의 깡통을 모았습니다. 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율은 얼마입니까?', 'chosen': '두 번째 날에 제임스가 수집한 깡통의 수를 첫 번째 날에 수집한 깡통의 수에 \\\\( x \\\\) 배로 표시해 봅시다. 따라서 둘째 날에 그는 \\\\( 50x \\\\) 깡통을 수집합니다.\\n\\n셋째 날에는 \\\\( 50x - 50 \\\\) 깡통을 수집합니다.\\n\\n남은 4일(4일부터 7일까지) 동안 그는 매일 50개의 깡통을 수집하여 총 4 \\\\times 50 = 200 개의 깡통을 수집합니다.\\n\\n한 주 동안 수집한 깡통의 총 개수는 매일 수집한 깡통의 합으로, 500개의 깡통과 같아야 합니다. 따라서 우리는 다음과 같습니다:\\n\\n\\\\[ 50 + 50x + (50x - 50) + 200 = 500 \\\\]\\n\\n유사한 항을 결합하면 다음과 같습니다:\\n\\n\\\\[ 50x + 50x + 200 = 500 \\\\]\\n\\n\\\\[ 100x + 200 = 500 \\\\]\\n\\n양쪽에서 200을 빼면 다음과 같습니다:\\n\\n\\\\[ 100x = 300 \\\\]\\n\\n양변을 100으로 나누면 다음과 같습니다:\\n\\n\\\\[ x = 3 \\\\]\\n\\n따라서 둘째 날에 수집한 깡통의 수와 첫째 날에 수집한 깡통의 수의 비율은 \\\\( x:1, 즉 3:1입니다.', 'rejected': ' 둘째 날에 제임스가 모은 깡통의 수를 \\\\( x \\\\)`로 표시해 봅시다.\\n\\n첫째 날에는 50개의 깡통을 모았습니다.\\n둘째 날에는 \\\\( x \\\\)개의 깡통을 모았습니다.\\n셋째 날에는 \\\\( x + 50 \\\\)개의 깡통을 모았습니다.\\n\\n일주일 동안 모은 깡통의 총 개수는 500개입니다. 따라서 방정식을 쓸 수 있습니다:\\n\\n\\\\[ 50 + x + (x + 50) + 50 + 50 + 50 = 500 \\\\]\\n\\n유사한 항을 결합하면 다음과 같습니다:\\n\\n\\\\[ 50 + x + x + 50 + 50 + 50 = 500 \\\\]\\n\\\\[ 250 + 2x = 500 \\\\]\\n\\n양쪽에서 250을 빼면 다음과 같습니다:\\n\\n\\\\[ 2x = 250 \\\\]\\n\\n양변을 2로 나누면 다음과 같습니다:\\n\\n\\\\[ x = 125 \\\\]\\n\\n따라서 제임스는 둘째 날에 125개의 깡통을 모았습니다.\\n\\n이제 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율을 구하기 위해 둘째 날의 깡통 수를 첫째 날의 깡통 수로 나눕니다:\\n\\n\\\\[ \\\\frac{x}{50} = \\\\frac{125}{50} \\\\]\\n\\n분수를 단순화하면 다음과 같습니다:\\n\\n\\\\[ \\\\frac{x}{50} = 2.5 \\\\]\\n\\n따라서 둘째 날에 모은 깡통 수와 첫째 날에 모은 깡통 수의 비율은 2.5:1입니다.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[9999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f8545",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bddb25d2",
   "metadata": {},
   "source": [
    "# 모델 준비\n",
    "\n",
    "https://huggingface.co/soka0000/vclm-korean-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5544f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 339/339 [00:03<00:00, 108.84it/s, Materializing param=model.norm.weight]                              \n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "source": [
    "# Policy(학습 대상) 모델/ 토크나이저 로드\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # 생성형 모델\n",
    "\n",
    "model_name = \"soka0000/vclm-korean-7b\"  # 모델 이름\n",
    "\n",
    "# 토크나이저 로드, trust_remote_code = 커스텀 코드 허용\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,  # 메모리 절약 및 연산 효율성을 높인다.\n",
    "    device_map=\"auto\",  # CPU/GPU 자동배치해준다.\n",
    "    trust_remote_code=True,  # 모델 저장소에 있는 커스텀 코드 허용\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c088be8",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e779ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 16084.60 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : ['system', 'question', 'chosen', 'rejected']\n",
      "after : ['system', 'question', 'chosen', 'rejected', 'prompt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 원본 샘플(system/question/chosen/rejected) -> prompt/chosen/rejected 형태의 모델용 학습 데이터 형태로 바꾸는 함수\n",
    "\n",
    "\n",
    "def preprocess_dpo_data(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example[\"system\"]},  # 시스템 프롬프트\n",
    "        {\"role\": \"user\", \"content\": example[\"question\"]},  # 사용자 프롬프트\n",
    "    ]\n",
    "\n",
    "    # 모델이 기대하는 포멧으로 변환 (|im_start|system...|im_start|user...)  답변 생성 시작 지점까지 모두 포함\n",
    "    prompt_style = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # DPOTrainer가 사용하기 좋은 형태\n",
    "    return {\n",
    "        \"prompt\": prompt_style,  # 템플릿이 적용된 프롬프트\n",
    "        \"chosen\": example[\"chosen\"],  # 선호 응답\n",
    "        \"rejected\": example[\"rejected\"],  # 비선호 응답\n",
    "    }\n",
    "\n",
    "\n",
    "dataset_preprocessed = dataset.map(\n",
    "    preprocess_dpo_data\n",
    ")  # 전체 데이터셋에 전처리 함수 적용\n",
    "\n",
    "print(f\"before : {dataset.column_names}\")\n",
    "print(f\"after : {dataset_preprocessed.column_names}\")  # prompt 컬럼은 추가 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4fe78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n당신은 인공지능 어시스턴트입니다.<|im_end|>\\n<|im_start|>user\\n정국이 5위입니다. 정국보다 결승선을 먼저 통과한 사람의 수를 찾아보세요.<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_preprocessed[\"prompt\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004fcfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 분할\n",
    "# - train/val/test 비율을 8:1:1\n",
    "\n",
    "train_size = int(len(dataset_preprocessed) * 0.8)\n",
    "val_size = int(len(dataset_preprocessed) * 0.1)\n",
    "test_size = int(len(dataset_preprocessed) * 0.1)\n",
    "train_dataset = dataset_preprocessed.select(range(train_size))\n",
    "val_dataset = dataset_preprocessed.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset_preprocessed.select(\n",
    "    range(train_size + val_size, train_size + val_size + test_size)\n",
    ")\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df1040d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로부터 텍스트 응답을 생성하는 함수\n",
    "def generate_response(model, tokenizer, quesion):\n",
    "    prompt = quesion  # 입력 질문을 그대로 프롬프트로 사용\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    # 텍스트를 토큰화해서 PyTorch 텐서로 변환 후, 모델이 있는 디바이스(CUDA/MPS/CPU)로 이동\n",
    "\n",
    "    with torch.no_grad():  # 추론 모드 (그래디언트 계산 비활성화 → 메모리/속도 절약)\n",
    "        outputs = model.generate(\n",
    "            **inputs,  # input_ids, attention_mask 등 전달\n",
    "            max_length=1024,  # 생성 포함 전체 최대 토큰 길이\n",
    "            do_sample=True,  # 샘플링 방식 사용 (확률 기반 생성)\n",
    "            top_k=50,  # 상위 50개 후보 중에서만 샘플링\n",
    "            top_p=0.95,  # 누적 확률 95% 안에서만 샘플링 (nucleus sampling)\n",
    "            temperature=0.5,  # 낮을수록 더 보수적/결정적 생성 창의성 0.5\n",
    "            num_return_sequences=1,  # 생성 결과 1개 반환\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 설정\n",
    "            pad_token_id=tokenizer.pad_token_id  # 패딩 토큰 설정\n",
    "        )\n",
    "\n",
    "        generate_text = tokenizer.decode(outputs[0])\n",
    "        # 생성된 토큰 시퀀스를 문자열로 디코딩\n",
    "\n",
    "        return generate_text.replace(prompt, \"\").strip()\n",
    "        # 프롬프트 부분 제거 후, 생성된 답변만 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41402fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔을 몇 개 모았고, 아비게일은 빈 캔 43개를 모았습니다. 그들은 빈 캔을 27개 더 모아야 합니다. 지금까지 알리사가 모은 빈 캔은 몇 개인가요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사가 얼마나 많은 빈 캔을 모았는지 알아내려면, 아비게일이 모은 캔의 수와 프로젝트에 필요한 총 캔 수에서 아직 모아야 할 캔의 수를 빼야 합니다.\n",
      "\n",
      "알리사의 캔 + 아비게일의 캔 + 아직 필요한 캔 = 필요한 총 캔 수입니다.\n",
      "알리사의 캔 = 필요한 총 캔 수 - (아비게일의 캔 + 아직 필요한 캔)\n",
      "\n",
      "우리는 이것을 알고 있습니다:\n",
      "필요한 총 캔 수 = 100\n",
      "아비게일의 캔 = 43\n",
      "아직 필요한 캔 = 27\n",
      "\n",
      "이제 숫자를 연결할 수 있습니다:\n",
      "\n",
      "알리사의 캔 = 100 - (43 + 27)\n",
      "알리사의 캔 = 100 - 70\n",
      "알리사의 캔 = 30\n",
      "\n",
      "알리사는 지금까지 빈 캔 30개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "현재까지 알리사가 모은 빈캔의 수를 구하려면, 총 필요한 빈캔의 수에서 아비게일이 모은 빈캔의 수를 빼고, 그 결과에서 아비게일이 더해야 하는 빈캔의 수를 빼야 해요. 하지만 문제는 두 사람이 빈캔을 함께 모으고 있다는 점을 고려해야 해요. 따라서, 알리사가 아비게일보다 더 많은 빈캔을 가져야 하므로, 먼저 아비게일이 더해야 하는 빈캔의 수를 빼야 해요. 아비게일이 더해야 하는 빈캔의 수: 27개 아비게일이 가져야 하는 빈캔의 수: 43개 이제, 두 사람 모두 더해야 하는 빈캔의 수를 빼요: 43 - 27 = 16개 이제, 이 값에서 알리사가 더해야 하는 빈캔의 수를 빼요: 16 - 27 = -11개 이 결과는 불가능해 보여요. 문제는 두 사람이 빈캔을 함께 모으고 있다는 점을 고려하지 않았다는 것을 의미해요. 만약 알리사가 더해야 하는 빈캔의 수를 빼면, 아비게일이 더해야 하는 빈캔의 수를 빼면, 두 사람이 더해야 하는 빈캔의 수가 27개라고 말할 수 있어요. 하지만, 그들이 빈캔을 함께 모으기 때문에, 그들의 합계를 더해야 해요. 두 사람이 더해야 하는 빈캔의 수를 더하면 다음과 같아요: 27 + 27 = 54개 이제, 아비게일이 이미 가져온 빈캔의 수를 빼야 해요: 54 - 43 = 11개 따라서, 현재까지 알리사가 가져온 빈캔의 수는 11개예요.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "알리사와 아비게일은 과학 프로젝트를 위해 빈 캔 100개를 모아야 합니다. 오늘 현재 알리사는 빈 캔 30개를 모았고 아비게일은 빈 캔을 몇 개 모았습니다. 두 사람은 빈 캔 27개를 더 모아야 합니다. 지금까지 아비게일은 몇 개의 빈 캔을 모았나요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      "알리사와 아비게일은 총 100개의 빈 캔을 모아야 합니다. 알리사는 이미 30개의 캔을 모았습니다. 100개의 목표에 도달하려면 27개의 캔을 더 모아야 합니다.\n",
      "\n",
      "아비게일이 얼마나 많은 캔을 모았는지 알아보려면, 알리사가 모은 캔의 수와 아직 모아야 할 캔의 수를 총 캔의 수에서 빼면 됩니다.\n",
      "\n",
      "따라서 아비게일이 모은 캔은 = 필요한 총 캔 수 - (알리사가 모은 캔 + 아직 모아야 할 캔)입니다.\n",
      "아비게일이 모은 캔 = 100 - (30 + 27)\n",
      "아비게일이 모은 캔 = 100 - 57\n",
      "아비게일이 모은 캔 = 43캔\n",
      "\n",
      "아비게일은 지금까지 빈 캔 43개를 모았습니다.\n",
      "\n",
      "[모델 생성 답변]:\n",
      "현재, 알리사가 30개의 빈캔을 모았다는 것을 알고 있으며, 그녀는 총 100개의 빈캔이 필요해요. 따라서, 그녀는 여전히 100 - 30 = 70개의 빈캔이 필요해요. 두 사람은 27개의 빈캔을 더 모으기로 합의했어요. 따라서, 그들이 지금까지 모은 총 빈캔 수는 30 + 27 = 57개가 돼요. 만약 알리사가 70개의 빈캔이 필요하다면, 그녀는 아비게일이 57 - 70 = -3개의 빈캔을 더 모았어야 해요. 이것은 불가능한 상황이므로, 아비게일이 아직 모으지 않은 빈캔의 수를 구하고 싶어하는 것이라고 가정할게요. 따라서, 아비게일은 57 - 30 = 27개의 빈캔을 모았을 거예요.<|im_end|>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[질문]:\n",
      "<|im_start|>system\n",
      "당신은 인공지능 어시스턴트입니다.<|im_end|>\n",
      "<|im_start|>user\n",
      "이반은 주사위 몇 개를 가지고 있습니다. 제리는 이반보다 두 배 많은 주사위를 가지고 있습니다. 두 사람은 총 60개의 주사위를 가지고 있습니다. 이반은 몇 개의 주사위를 가지고 있을까요?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "[정답]:\n",
      " 이반이 가지고 있는 주사위의 수를 \\( x \\)`로 표시해 봅시다. 문제에 따르면 제리는 이반보다 두 배 많은 주사위를 가지고 있으므로 제리는 \\( 2x \\) 주사위를 가지고 있습니다.\n",
      "\n",
      "두 사람이 함께 가지고 있는 주사위의 총 개수는 60개입니다. 따라서 방정식을 쓸 수 있습니다:\n",
      "\n",
      "\\[ x + 2x = 60 \\]\n",
      "\n",
      "유사한 항을 결합하면 다음과 같습니다:\n",
      "\n",
      "\\[ 3x = 60 \\]\n",
      "\n",
      "이제 방정식의 양쪽을 3으로 나누어 \\( x \\)`를 풀 수 있습니다:\n",
      "\n",
      "\\[ x = \\frac{60}{3} \\]\n",
      "\n",
      "\\[ x = 20 \\]\n",
      "\n",
      "따라서 이반은 20개의 주사위를 가지고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "test_subset = test_dataset.select(range(3))\n",
    "\n",
    "for i, example in enumerate(test_subset):  # 샘플 3개를 순회\n",
    "    quesiton = example[\"prompt\"]\n",
    "    answer = example[\"chosen\"]  # 선호 답변(데이터셋에서는 정답)\n",
    "\n",
    "    print(f\"[질문]:\\n{quesiton}\")\n",
    "    print(f\"\\n[정답]:\\n{answer}\")\n",
    "\n",
    "    generated_answer = generate_response(model, tokenizer, quesiton)\n",
    "    print(f\"\\n[모델 생성 답변]:\\n{generated_answer}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bcd63",
   "metadata": {},
   "source": [
    "# DPO 학습준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA(PEFT) 적용으로 정책모델 학습 파라미터 축소 확인\n",
    "from peft import LoraConfig, get_peft_model  # LoRA 설정 / PEFT 적용함수\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 랭크(추가로 학습할 저차원 행렬 크기)\n",
    "    lora_alpha=16,  # LoRA 스케일 (업데이트 강도 조절)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # LoRA에 붙일 모듈(어텐션의 Q/V 프로젝션)\n",
    "    lora_dropout=0.05,  # 과적합 완화\n",
    "    bias=\"none\",  # Bias 학습 제외\n",
    "    task_type=\"CAUSAL_LM\",  # 작업 유형(디코더 LM)\n",
    ")\n",
    "\n",
    "# 정책 모델(학습 대상)\n",
    "model = get_peft_model(model, lora_config)  # 기존 모델에 LoRA 적용 (PEFT 모델)\n",
    "\n",
    "model.print_trainable_parameters()  # 학습 가능한 파라미터 수/비율 체크!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참조모델 : 정책모델이 얼마나 바뀌었는지 비교 기준이 되는 고정 모델\n",
    "# - chosen/rejected 답변에 대한 생성 확률 비교값 제공\n",
    "\n",
    "\n",
    "# 참조모델 (학습하지 않은 기본모델)\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,  # 메모리 절약 및 연산 효율성을 높인다.\n",
    "    device_map=\"auto\",  # CPU/GPU 자동배치해준다.\n",
    "    trust_remote_code=True,  # 모델 저장소에 있는 커스텀 모델 코드 허용\n",
    ")\n",
    "\n",
    "\n",
    "# 학습방지 처리\n",
    "reference_model.eval()  # 평가모드 전환\n",
    "for param in reference_model.parameters():  # 모든 파라미터 순회\n",
    "    param.requires_grad = False  # gradient 계산 / 업데이트 방지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc77d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPOConfig 설정 및 DPOTrainer로 학습 실행\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "hub_model_id = (\n",
    "    \"SEUNGHUN12/vclm-korean-7b-orca-math-korean-dpo\"  # 학습 완료 모델을 올릴 hub 저장소\n",
    ")\n",
    "\n",
    "\n",
    "# DPO 학습 하이퍼파라미터/로깅/저장 설정\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"vclm-korean-7b-orca-math-korean-dpo\",  # 체크포인트/로그 저장 폴더\n",
    "    num_train_epochs=1,  # 전체 데이터를 1번 반복 학습\n",
    "    per_device_train_batch_size=2,  # GPU 1개당 배치 크기\n",
    "    # 4번 누적 후 업데이트(실제 배치 효과: 2*4=8)\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,  # 학습률\n",
    "    eval_strategy=\"steps\",  # 일정 step마다 평가 수행\n",
    "    save_strategy=\"steps\",  # 일정 step마다 저장 수행\n",
    "    logging_steps=50,  # 50 step마다 학습 로그 출력\n",
    "    # fp16 비활성화(여기서는 bf16 사용)\n",
    "    fp16=False,\n",
    "    # bfloat16 사용(지원 GPU에서 안정적/빠름)\n",
    "    bf16=True,\n",
    "    # Ampere 이상에서 matmul 가속 옵션(정밀도 약간 완화)\n",
    "    tf32=True,\n",
    "    beta=0.1,  # DPO의 beta(선호 강도 조절)\n",
    "    max_length=512,  # prompt+답변을 포함한 최대 길이\n",
    "    max_prompt_length=256,  # prompt에 할당할 최대 길이\n",
    "    remove_unused_columns=False,  # DPO에 필요한 컬럼이 제거되지 않도록 유지\n",
    "    push_to_hub=True,  # 학습 결과를 Hugging Face Hub로 업로드\n",
    "    hub_model_id=hub_model_id,  # 업로드할 저장소 이름\n",
    "    hub_strategy=\"end\",  # 학습 끝난 뒤 한 번만 업로드\n",
    "    report_to=[\"wandb\"],  # wandb로 학습 로그 전송\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Trainer 생성(정책모델 vs 참조모델 비교 학습)\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,  # 정책모델\n",
    "    ref_model=reference_model,  # 참조모델\n",
    "    args=training_args,  # 학습 설정\n",
    "    train_dataset=train_dataset,  # 학습 데이터 (prompt/chosen/rejected)\n",
    "    eval_dataset=val_dataset,  # 검증 데이터\n",
    "    processing_class=tokenizer,  # 토큰화 처리\n",
    ")\n",
    "\n",
    "dpo_trainer.train()  # DPO 학습 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb716730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습이 중간에 끊어졌거나 혹은 자동으로 업로드되지 않은 경우\n",
    "# dpo_trainer.push_to_hub('Commit!') # 현재 학습된 체크포인트/모델을 Hub에 커밋 메시지와 함께 업로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb3b82",
   "metadata": {},
   "source": [
    "# DPO Model 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2922c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = test_dataset.select(range(3))\n",
    "\n",
    "for i, example in enumerate(test_subset):  # 샘플 3개를 순회\n",
    "    quesiton = example[\"prompt\"]\n",
    "    answer = example[\"chosen\"]  # 선호 답변(데이터셋에서는 정답)\n",
    "\n",
    "    print(f\"[질문]:\\n{quesiton}\")\n",
    "    print(f\"\\n[정답]:\\n{answer}\")\n",
    "\n",
    "    generated_answer = generate_response(model, tokenizer, quesiton)\n",
    "    print(f\"\\n[모델 생성 답변]:\\n{generated_answer}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a3645",
   "metadata": {},
   "source": [
    "## DPO의 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# prompt에 대해 response가 얼마나 그럴듯한지(평균 log_prob) 계산하는 함수\n",
    "\n",
    "\n",
    "def calculate_log_prob(model, tokenizer, prompt, response):\n",
    "    \"\"\"\n",
    "    주어진 prompt에 대한 response의 log probability(평균)를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        model: HuggingFace AutoModelForCausalLM (or similar)\n",
    "        tokenizer: HuggingFace AutoTokenizer\n",
    "        prompt (str): 프롬프트 텍스트\n",
    "        response (str): 응답 텍스트\n",
    "\n",
    "    Returns:\n",
    "        float: response 토큰들의 평균 log probability\n",
    "    \"\"\"\n",
    "    full_text = prompt + \" \" + response  # Prompt + 답변\n",
    "    inputs = tokenizer(\n",
    "        full_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,  # 최대 글자 길이 이상시 자름\n",
    "        max_length=512,  # 최대 토큰 길이\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    prompt_tokens = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    ).to(model.device)\n",
    "    prompt_len = prompt_tokens[\"input_ids\"].shape[1]\n",
    "\n",
    "    # 추론 모드(그래프 미생성/ 계산만 진행) : 메모리가 절약\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = logits[..., 1:].contiguous()\n",
    "\n",
    "    log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "    true_log_probs = torch.gather(log_probs, 2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    seq_len = shift_labels.shape[1]\n",
    "    if prompt_len >= seq_len:\n",
    "        valid_log_probs = true_log_probs[:, -1:]\n",
    "    else:\n",
    "        start_idx = max(0, prompt_len - 1)\n",
    "        valid_log_probs = true_log_probs[:, start_idx:]\n",
    "\n",
    "    avg_log_probs = valid_log_probs.mean().item()\n",
    "\n",
    "    return avg_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcualte_preference_accuracy(model, tokenizer, dataset, num_samples=100):\n",
    "    \"\"\"\n",
    "    모델이 선호(chosen) 답변에 비선호(rejected) 답변 보다 더 높은 확률을 부여하는지를 평가\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    total = min(num_samples, len(dataset))\n",
    "\n",
    "    print(f\"{total}개 샘플에 대해 정확도 계산 시작\")\n",
    "\n",
    "    model.eval()\n",
    "    for idx in range(total):\n",
    "        example = dataset[idx]\n",
    "\n",
    "        prompt = example[\"prompt\"]\n",
    "        chosen = example[\"chosen\"]\n",
    "        rejected = example[\"rejected\"]\n",
    "\n",
    "        chosen_score = calculate_log_prob(model, tokenizer, prompt, chosen)\n",
    "        rejected_score = calculate_log_prob(model, tokenizer, prompt, rejected)\n",
    "\n",
    "        if chosen_score > rejected_score:\n",
    "            correct += 1\n",
    "\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\" - 진행률 : {idx+1}{total} (현재 정확도: {correct/(idx+1)*100:.2f}%)\"\n",
    "            )\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "test_accuracy = calcualte_preference_accuracy(\n",
    "    model, tokenizer, dataset, num_samples=300\n",
    ")\n",
    "print(test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}