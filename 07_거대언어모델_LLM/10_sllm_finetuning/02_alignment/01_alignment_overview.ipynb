{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdz4ZfxayzLlvUjNB3VNT4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Aligment\n","\n","## 1. RLHF (Reinforcement Learning from Human Feedback)\n","\n","AI를 인간의 의도와 선호에 맞게 정렬(Alignment)하는 가장 널리 알려진 방식이다. 단순히 사람의 말을 흉내 내는 것을 넘어, 사람이 '더 좋아하는' 응답을 하도록 훈련한다.\n","\n","* **작동 원리**:\n","1. **선호도 수집**: 사람이 AI의 답변들을 비교하여 더 좋은 답변을 선택한다.\n","2. **보상 모델 훈련**: 사람의 선택 기준을 학습한 보상 함수(Reward Function)를 만든다.\n","3. **강화학습(RL) 적용**: 보상 모델을 통해 점수를 최대화하도록 PPO 알고리즘 등을 사용해 AI를 조정한다.\n","\n","\n","* **한계**: 과정이 복잡하고 불안정하며, 별도의 보상 모델 훈련이 필요해 비용이 많이 든다.\n","\n","\n","\n","## 2. DPO (Direct Preference Optimization, 직접 선호 최적화)\n","\n","복잡한 강화학습 과정을 생략하고, 사람의 선호 데이터를 이용해 모델을 직접 훈련하는 방식이다.\n","\n","* **핵심 아이디어**: 별도의 보상 모델이나 강화학습(PPO) 없이, 선호도 데이터를 손실 함수(Loss Function)에 직접 반영하여 학습한다.\n","* **작동 방식**:\n","* 사람이 선호한 응답(Winner)과 덜 선호한 응답(Loser)을 비교한다.\n","* 모델이 Winner를 선택할 확률은 높이고 Loser를 선택할 확률은 낮추도록 유도하는 **DPO 손실 함수**를 사용한다.\n","\n","\n","* **장점**:\n","* 과정이 단순하고 훈련이 안정적이다.\n","* RLHF와 비슷한 성능을 내면서도 구현이 쉽다.\n","\n","\n","* **단점**: 새로운 답을 탐색(Exploration)하는 능력은 상대적으로 부족하며 유연성이 떨어진다.\n","\n","\n","\n","## 3. RRHF (Reward-Rank Hindsight Fine-Tuning, 보상-순위 기반 사후 파인튜닝)\n","\n","순위(Rank)를 매기는 방식과 사후 판단(Hindsight)을 결합하여 모델을 미세 조정하는 기법이다.\n","\n","* **핵심 아이디어**: 생성된 답변들에 대해 보상 점수를 매기고, 그 점수에 맞춰 모델을 정렬한다. PPO처럼 복잡한 4개의 모델을 띄울 필요가 없다.\n","* **작동 방식**:\n","* 다양한 응답을 생성하고 보상 모델로 점수를 매겨 순위를 정한다.\n","* 높은 점수를 받은 응답을 학습하도록 모델을 업데이트한다.\n","* 학습 중에 KL 발산(KL-Divergence) 페널티를 계산할 필요가 없다.\n","\n","\n","* **장점**:\n","* PPO 대비 메모리 사용량이 적고 효율적이다 (1~2개 모델만 필요).\n","* 단일 턴 대화 등에서 높은 성능을 기록했다.\n","\n","\n","* **단점**:\n","* 여전히 보상 모델의 품질에 크게 의존한다.\n","* 학습 과정에서 여러 응답을 동시에 처리해야 하므로 순간적인 GPU 메모리 소모가 클 수 있다.\n","\n","\n","\n","\n","\n","## 4. RLAIF (Reinforcement Learning from AI Feedback, AI 피드백 기반 강화학습)\n","\n","인간 대신 AI가 피드백을 제공하여 강화학습을 수행하는 방식이다. '사람의 평가'를 'AI의 평가'로 대체한다.\n","\n","* **핵심 아이디어**: 사람이 데이터를 일일이 만드는 것은 비싸고 느리므로, 범용 LLM(거대언어모델)이 사람 대신 선호도를 판단하게 한다.\n","* **작동 방식**:\n","* 평가자 역할을 하는 AI에게 두 답변 중 더 나은 것을 고르도록 프롬프트를 입력한다.\n","* AI가 생성한 선호도 데이터를 바탕으로 강화학습을 진행한다.\n","\n","\n","* **장점**:\n","* 인간 데이터 수집 비용과 시간을 획기적으로 절감한다.\n","* 대규모 확장이 용이하다.\n","\n","\n","* **단점**:\n","* 평가자 AI의 성능과 프롬프트 구성에 따라 결과가 크게 달라질 수 있다.\n","* AI의 편향이 강화될 위험이 있다.\n","\n","\n","\n","\n","\n","## 5. 요약 및 결론\n","\n","각 방법은 상황과 목적에 따라 선택해서 사용해야 한다.\n","\n","| 구분 | 추천 상황 | 특징 |\n","| --- | --- | ---  |\n","| **DPO** | 고품질 선호도 데이터가 이미 있을 때 | 가장 단순하고 빠르며 안정적이다. |\n","| **RRHF** | 다양한 응답을 유연하게 다루고 싶을 때 | PPO의 무거운 대체제로 적합하며 효율적이다. |\n","| **RLAIF** | 인간 평가자를 구하기 어렵거나 비용을 줄일 때 | 대규모 학습에 유리하며 AI가 스스로를 가르친다. |\n","\n","결국 이 모든 기술은 AI가 사람의 가치와 의도를 더 잘 이해하고 따르도록 만들기 위한 '정렬(Alignment)' 기술의 발전 과정이다."],"metadata":{"id":"YNiM1Jt9jvsO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RY81mDPxjcb3"},"outputs":[],"source":[]}]}